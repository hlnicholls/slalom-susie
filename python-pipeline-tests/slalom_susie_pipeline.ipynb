{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import re\n",
    "import scipy as sp\n",
    "\n",
    "# Set input variables\n",
    "gwas_file_path = \"/Users/hn9/Documents/Analysis/FM-comparison/gwas-examples/APOE-LDL/24097068-GCST002222-EFO_0004611.h.tsv.gz\"\n",
    "target = \"APOE_LDL\"\n",
    "target_chrom = 19\n",
    "target_pos = 44908822\n",
    "start_pos = target_pos - 500000\n",
    "end_pos = target_pos + 500000\n",
    "lead_snp_ID = f\"{target_chrom}:{target_pos}:C:T\"\n",
    "n_sample = 94595"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ld_matrix():\n",
    "    # Calculate LD correlation\n",
    "    ld_data = pd.read_csv(f\"{target}_locus_UKBB.txt.raw\", delim_whitespace=True)\n",
    "    ld_data = ld_data.drop(columns=[\"FID\", \"IID\", \"PAT\", \"MAT\", \"SEX\", \"PHENOTYPE\"])\n",
    "    ld_matrix = pd.DataFrame(np.corrcoef(df.values, rowvar=False), columns=df.columns)\n",
    "    return ld_data, ld_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ld_matrix():\n",
    "    # Calculate LD correlation\n",
    "    ld_data = pd.read_csv(f\"{target}_locus_UKBB.txt.raw\", delim_whitespace=True)\n",
    "    ld_data = ld_data.drop(columns=[\"FID\", \"IID\", \"PAT\", \"MAT\", \"SEX\", \"PHENOTYPE\"])\n",
    "    ld_matrix = ld_data.corr(method='pearson')\n",
    "    return ld_data, ld_matrix\n",
    "\n",
    "\n",
    "def get_sumstats(gwas_file_path, target_chrom, start_pos, end_pos):\n",
    "    #Â Preparing sumstats for filtering and input into SuSiE\n",
    "    sumstat = pd.read_csv(gwas_file_path, sep=\"\\t\")\n",
    "    sumstat = sumstat[(sumstat['hm_chrom'] == target_chrom) & (sumstat['hm_pos'] >= start_pos) & (sumstat['hm_pos'] <= end_pos)]\n",
    "    sumstat = sumstat.dropna(subset=['hm_chrom'])\n",
    "    sumstat['z'] = sumstat['beta'] / sumstat['standard_error']\n",
    "    cols_to_rename = {\n",
    "        'hm_variant_id': 'ID',\n",
    "        'hm_rsid': 'rsid',\n",
    "        'hm_chrom': 'chromosome',\n",
    "        'hm_pos': 'position',\n",
    "        'hm_other_allele': 'allele1',\n",
    "        'hm_effect_allele': 'allele2',\n",
    "        'hm_effect_allele_frequency': 'maf',\n",
    "        'standard_error': 'se',\n",
    "        'p_value': 'p'\n",
    "    }\n",
    "    sumstat.rename(columns=cols_to_rename, inplace=True)\n",
    "    sumstat = sumstat[['ID', 'rsid', 'chromosome', 'position', 'allele1', 'allele2', 'maf', 'p', 'beta', 'se', 'z']]\n",
    "    return sumstat\n",
    "\n",
    "\n",
    "def match_snps(sumstat, ld_matrix, ld_data):\n",
    "    # Getting only SNPs in sumstats that are in the LD matrix\n",
    "    # Get SNP IDs from ld matrix to compare with sumstat ID\n",
    "    pattern = re.compile(r\"(^\\d+)|(?<=:)\\d+(?=:|$)\")\n",
    "    df1_transpose = ld_data.T.reset_index()\n",
    "    df1_transpose.columns = ['SNP'] + list(df1_transpose.columns[1:])\n",
    "    df1_transpose['position'] = df1_transpose['SNP'].apply(lambda x: re.search(pattern, x).group())\n",
    "    df1_transpose['ID'] = df1_transpose['SNP'].str.replace(r'[:,_]', '_').str.replace(r'_[^_]+$', '')\n",
    "    concordance_test = pd.merge(sumstat, df1_transpose, on='ID')\n",
    "\n",
    "    # Filter sumstat and LD matrix for matches only\n",
    "    sumstat_filtered = sumstat[sumstat['ID'].isin(concordance_test['ID'])]\n",
    "    sumstat_filtered.reset_index(drop=True, inplace=True)\n",
    "    ld_matrix_filtered = ld_matrix.loc[concordance_test['SNP'], concordance_test['SNP']]\n",
    "    return sumstat_filtered, ld_matrix_filtered, concordance_test\n",
    "\n",
    "\n",
    "def allele_flip_check(concordance_test, sumstat_filtered, ld_matrix_filtered):\n",
    "    # Create DataFrame for allele flip check\n",
    "    concordance_test2 = concordance_test.copy()\n",
    "\n",
    "    # Extract alleles\n",
    "    allele_df = concordance_test2['SNP'].str.extract(r'[:,_]([ACGT]+)[:,_]([ACGT]+)')\n",
    "    concordance_test2['allele1_LD'] = allele_df[0]\n",
    "    concordance_test2['allele2_LD'] = allele_df[1]\n",
    "\n",
    "    # Make sure indices are aligned before the next operation\n",
    "    sumstat_filtered.index = concordance_test2.index\n",
    "\n",
    "    # Flip z-scores if alleles are discordant\n",
    "    sumstat_filtered['z'] = np.where(\n",
    "        (sumstat_filtered['allele1'] != concordance_test2['allele1_LD']) | (sumstat_filtered['allele2'] != concordance_test2['allele2_LD']),\n",
    "        -sumstat_filtered['z'], sumstat_filtered['z']\n",
    "    )\n",
    "\n",
    "    # Check for NaNs or Inf values (not accepted in fine-mapping)\n",
    "    numeric_cols = sumstat_filtered.select_dtypes(include=[np.number])\n",
    "    any_na_matrix = ld_matrix_filtered.isna().any().any()\n",
    "    any_inf_matrix = np.isinf(ld_matrix_filtered.to_numpy()).any()\n",
    "    any_na_sumstat = numeric_cols.isna().any().any()\n",
    "    any_inf_sumstat = np.isinf(numeric_cols.to_numpy()).any()\n",
    "\n",
    "    print(f\"Correlation matrix has NaNs: {any_na_matrix}, Infs: {any_inf_matrix}\")\n",
    "    print(f\"Sumstat has NaNs: {any_na_sumstat}, Infs: {any_inf_sumstat}\")\n",
    "    ld_matrix_filtered.to_csv(f\"{target}_locus_ukbb_ld.txt.gz\", sep='\\t', index=False, header=False)\n",
    "    sumstat_filtered.to_csv(f\"{target}_locus_sumstat_flip_check.txt.gz\", sep='\\t', index=False)\n",
    "\n",
    "    return sumstat_filtered\n",
    "\n",
    "\n",
    "def outlier_detection(sumstat, method, r2_threshold = 0.6, nlog10p_dentist_s_threshold = 1e-4):\n",
    "    if method == 'DENTIST':\n",
    "        # 1. Getting R2 column for sumstats\n",
    "        # LD for all variants with lead SNP previously calculated by plink\n",
    "        ld = pd.read_csv(f'/Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/ld/{target}_subset_for_ld_calculation.ld', sep='\\s+')\n",
    "        lead_ld = ld[(ld['SNP_A'] == f'{lead_snp_ID}') | (ld['SNP_B'] == f'{lead_snp_ID}')]\n",
    "        sumstat = pd.read_csv(f'/Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/{target}_locus_sumstat_flip_check.txt.gz', sep='\\t')\n",
    "        sumstat['ID'] = sumstat['ID'].str.replace(\"_\", \":\")\n",
    "        merged = pd.merge(lead_ld, sumstat[['ID']], left_on='SNP_B', right_on='ID')\n",
    "        df = merged[['ID', 'R2']]\n",
    "        df = pd.merge(sumstat, merged, on='ID', how='left')\n",
    "        df['r'] = (n_sample * df['R2'].sum()) / (n_sample * df['R2'].count())\n",
    "        lead = df[df['ID'] == lead_snp_ID].iloc[0]\n",
    "        lead_idx_snp = df.index[df['ID'] == lead_snp_ID].tolist()[0]\n",
    "\n",
    "        # 2. Calculate 't_dentist_s' and 'dentist_outlier'\n",
    "        # Copied and unaltered from https://github.com/mkanai/slalom/blob/master/slalom.py\n",
    "        lead_z = (df.beta / df.se).iloc[lead_idx_snp]\n",
    "        df[\"t_dentist_s\"] = ((df.beta / df.se) - df.r * lead_z) ** 2 / (1 - df.r ** 2)\n",
    "        df[\"t_dentist_s\"] = np.where(df[\"t_dentist_s\"] < 0, np.inf, df[\"t_dentist_s\"])\n",
    "        df[\"t_dentist_s\"].iloc[lead_idx_snp] = np.nan\n",
    "        df[\"nlog10p_dentist_s\"] = sp.stats.chi2.logsf(df[\"t_dentist_s\"], df=1) / -np.log(10)\n",
    "        n_dentist_s_outlier = np.sum(\n",
    "                    (df.R2 > r2_threshold) & (df.nlog10p_dentist_s > nlog10p_dentist_s_threshold)\n",
    "                )\n",
    "        print('Number of DENTIST outliers detected:', n_dentist_s_outlier)\n",
    "        # Identifying outliers\n",
    "        df['dentist_outlier'] = np.where((df.R2 > r2_threshold) & (df.nlog10p_dentist_s > nlog10p_dentist_s_threshold), 1, 0)\n",
    "        df = df.drop(['CHR_A', 'BP_A', 'SNP_A', 'CHR_B', 'BP_B', 'SNP_B'], axis=1)\n",
    "        df.to_csv(f'{target}_locus_sumstat_with_dentist.txt.gz', sep='\\t', index=False)\n",
    "    elif method == 'CARMA':\n",
    "        # unfinished work in progress\n",
    "        from scipy import optimize\n",
    "        import carmapy.carmapy_c\n",
    "        sumstats = pd.read_csv(f\"{target}_locus_sumstat_flip_check.txt.gz\", sep='\\t')\n",
    "        ld = pd.read_csv(f\"{target}_locus_ukbb_ld.txt.gz\", sep='\\t', header=None)\n",
    "        outlier_tau = 0.04\n",
    "        index_list = sumstats.index.tolist()\n",
    "        z = sumstats['z'].tolist()\n",
    "        ld_matrix = np.asmatrix(ld)\n",
    "        modi_ld_S = ld_matrix\n",
    "        def ridge_fun(x, modi_ld_S, index_list, temp_Sigma, z, outlier_tau, outlier_likelihood):\n",
    "            temp_ld_S = x * modi_ld_S + (1 - x) * np.eye(modi_ld_S.shape[0])\n",
    "            ld_matrix[index_list[:, None], index_list] = temp_ld_S\n",
    "            return outlier_likelihood(index_list, ld_matrix, z, outlier_tau, len(index_list), 1)\n",
    "\n",
    "        opizer = optimize(ridge_fun, interval=[0, 1], maximum=True)\n",
    "        modi_ld = opizer['maximum'] * modi_ld_S + (1 - opizer['maximum']) * np.diag(np.diag(modi_ld_S))\n",
    "        outlier_likelihood = carmapy.carmapy_c.outlier_Normal_fixed_sigma_marginal\n",
    "        test_log_BF = outlier_likelihood(index_list, ld_matrix, z, outlier_tau, len(index_list), 1) - outlier_likelihood(index_list, modi_ld, z, outlier_tau, len(index_list), 1)\n",
    "        test_log_BF = -abs(test_log_BF)\n",
    "        print('Outlier BF:', test_log_BF)\n",
    "        print('This is xi hat:', opizer)\n",
    "    else:\n",
    "        pass\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLINK v1.90b6.21 64-bit (19 Oct 2020)          www.cog-genomics.org/plink/1.9/\n",
      "(C) 2005-2020 Shaun Purcell, Christopher Chang   GNU General Public License v3\n",
      "Logging to APOE_LDL_locus_UKBB.txt.log.\n",
      "Options in effect:\n",
      "  --allow-extra-chr\n",
      "  --bfile /Users/hn9/Documents/Analysis/FM-comparison/ukb_v3_downsampled10k/ukb_v3_chr19.downsampled10k\n",
      "  --chr 19\n",
      "  --from-bp 44408822\n",
      "  --maf 0.001\n",
      "  --out APOE_LDL_locus_UKBB.txt\n",
      "  --recode A\n",
      "  --to-bp 45408822\n",
      "\n",
      "16384 MB RAM detected; reserving 8192 MB for main workspace.\n",
      "6167 out of 364540 variants loaded from .bim file.\n",
      "10000 people (0 males, 0 females, 10000 ambiguous) loaded from .fam.\n",
      "Ambiguous sex IDs written to APOE_LDL_locus_UKBB.txt.nosex .\n",
      "Using 1 thread (no multithreaded calculations invoked).\n",
      "Before main variant filters, 10000 founders and 0 nonfounders present.\n",
      "Calculating allele frequencies... 10111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989 done.\n",
      "Total genotyping rate is 0.999988.\n",
      "0 variants removed due to minor allele threshold(s)\n",
      "(--maf/--max-maf/--mac/--max-mac).\n",
      "6167 variants and 10000 people pass filters and QC.\n",
      "Note: No phenotypes present.\n",
      "--recode A to APOE_LDL_locus_UKBB.txt.raw ... 101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899done.\n",
      "Correlation matrix has NaNs: False, Infs: False\n",
      "Sumstat has NaNs: False, Infs: False\n",
      "PLINK v1.90b6.21 64-bit (19 Oct 2020)          www.cog-genomics.org/plink/1.9/\n",
      "(C) 2005-2020 Shaun Purcell, Christopher Chang   GNU General Public License v3\n",
      "Logging to /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/ld/APOE_LDL_subset_for_ld_calculation.log.\n",
      "Options in effect:\n",
      "  --allow-extra-chr\n",
      "  --bfile /Users/hn9/Documents/Analysis/FM-comparison/ukb_v3_downsampled10k/ukb_v3_chr19.downsampled10k\n",
      "  --ld-snp 19:44908822:C:T\n",
      "  --ld-window 99999\n",
      "  --ld-window-kb 1000\n",
      "  --ld-window-r2 0\n",
      "  --out /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/ld/APOE_LDL_subset_for_ld_calculation\n",
      "  --r2\n",
      "\n",
      "16384 MB RAM detected; reserving 8192 MB for main workspace.\n",
      "364540 variants loaded from .bim file.\n",
      "10000 people (0 males, 0 females, 10000 ambiguous) loaded from .fam.\n",
      "Ambiguous sex IDs written to\n",
      "/Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/ld/APOE_LDL_subset_for_ld_calculation.nosex\n",
      ".\n",
      "Using up to 8 threads (change this with --threads).\n",
      "Before main variant filters, 10000 founders and 0 nonfounders present.\n",
      "Calculating allele frequencies... 10111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989 done.\n",
      "Total genotyping rate is 0.999981.\n",
      "364540 variants and 10000 people pass filters and QC.\n",
      "Note: No phenotypes present.\n",
      "--r2 to\n",
      "/Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/ld/APOE_LDL_subset_for_ld_calculation.ld\n",
      "... 0% [processingwriting]          done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mp/mm_rrrpj4cj0sf0lspp7vmvm0000gs/T/ipykernel_33708/2415294170.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"t_dentist_s\"].iloc[lead_idx_snp] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of DENTIST outliers detected: 1\n"
     ]
    }
   ],
   "source": [
    "# PLINK command to get LD matrix (placeholder using PLINK and UKBiobank LD reference panel)\n",
    "command = f\"plink --bfile /Users/hn9/Documents/Analysis/FM-comparison/ukb_v3_downsampled10k/ukb_v3_chr{target_chrom}.downsampled10k --allow-extra-chr --recode A --chr {target_chrom} --from-bp {start_pos} --to-bp {end_pos} --maf 0.001 --out {target}_locus_UKBB.txt\"\n",
    "subprocess.run(command, shell=True)\n",
    "\n",
    "# Run functions\n",
    "ld_data, ld_matrix = get_ld_matrix()\n",
    "sumstat = get_sumstats(gwas_file_path, target_chrom, start_pos, end_pos)\n",
    "sumstat_filtered, ld_matrix_filtered, concordance_test = match_snps(sumstat, ld_matrix, ld_data)\n",
    "sumstat_filtered = allele_flip_check(concordance_test, sumstat_filtered, ld_matrix_filtered)\n",
    "\n",
    "# PLINK command to get SNPs in LD with lead SNP\n",
    "lead_ld_command = f\"\"\"plink --bfile /Users/hn9/Documents/Analysis/FM-comparison/ukb_v3_downsampled10k/ukb_v3_chr{target_chrom}.downsampled10k \\\n",
    "        --allow-extra-chr \\\n",
    "        --r2 \\\n",
    "        --ld-snp {lead_snp_ID} \\\n",
    "        --ld-window-kb 1000 \\\n",
    "        --ld-window 99999 \\\n",
    "        --ld-window-r2 0 \\\n",
    "        --out /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/ld/{target}_subset_for_ld_calculation\n",
    "\"\"\"\n",
    "subprocess.run(lead_ld_command, shell=True)\n",
    "\n",
    "# Function to calculate DENTIST outlier detection\n",
    "df = outlier_detection(sumstat_filtered, method='DENTIST')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/hn9/Documents/GitHub/fine-mapping-inf/run_fine_mapping.py\", line 7, in <module>\n",
      "    import bgzip\n",
      "  File \"/Users/hn9/anaconda3/envs/finemap_env/lib/python3.9/site-packages/bgzip/__init__.py\", line 7, in <module>\n",
      "    from bgzip import bgzip_utils  # type: ignore\n",
      "ImportError: dlopen(/Users/hn9/anaconda3/envs/finemap_env/lib/python3.9/site-packages/bgzip/bgzip_utils.cpython-39-darwin.so, 0x0002): symbol not found in flat namespace '___kmpc_barrier'\n",
      "gunzip: can't stat: APOE_LDL_locus.susieinf.bgz (APOE_LDL_locus.susieinf.bgz.gz): No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='gunzip -c APOE_LDL_locus.susieinf.bgz > APOE_LDL_locus.txt', returncode=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SuSiE fine-mapping with fine-mapping-inf package\n",
    "susieinf_command = f\"\"\"python /Users/hn9/Documents/GitHub/fine-mapping-inf/run_fine_mapping.py \\\n",
    "    --sumstats /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/{target}_locus_sumstat_with_dentist.txt.gz \\\n",
    "    --beta-col-name beta \\\n",
    "    --se-col-name se \\\n",
    "    --ld-file /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/{target}_locus_ukbb_ld.txt.gz \\\n",
    "    --n {n_sample} \\\n",
    "    --method susieinf \\\n",
    "    --save-tsv \\\n",
    "    --eigen-decomp-prefix {target}_locus \\\n",
    "    --output-prefix  {target}_locus \"\"\"\n",
    "\n",
    "subprocess.run(susieinf_command, shell=True)\n",
    "\n",
    "zip_comamnd = f\"\"\"gunzip -c {target}_locus.susieinf.bgz > {target}_locus.txt\"\"\"\n",
    "subprocess.run(zip_comamnd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DENTIST Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mp/mm_rrrpj4cj0sf0lspp7vmvm0000gs/T/ipykernel_12730/1863148979.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"t_dentist_s\"].iloc[lead_idx_snp] = np.nan\n"
     ]
    }
   ],
   "source": [
    "# 1. Getting R2 column for sumstats\n",
    "# LD for all variants with lead SNP previously calculated by plink\n",
    "ld = pd.read_csv(f'/Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/ld/{target}_subset_for_ld_calculation.ld', sep='\\s+')\n",
    "lead_ld = ld[(ld['SNP_A'] == f'{lead_snp_ID}') | (ld['SNP_B'] == f'{lead_snp_ID}')]\n",
    "sumstat = pd.read_csv(f'/Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/{target}_locus_sumstat_flip_check.txt.gz', sep='\\t')\n",
    "sumstat['ID'] = sumstat['ID'].str.replace(\"_\", \":\")\n",
    "merged = pd.merge(lead_ld, sumstat[['ID']], left_on='SNP_B', right_on='ID')\n",
    "df = merged[['ID', 'R2']]\n",
    "df = pd.merge(sumstat, merged, on='ID', how='left')\n",
    "df['r'] = (n_sample * df['R2'].sum()) / (n_sample * df['R2'].count())\n",
    "lead = df[df['ID'] == lead_snp_ID].iloc[0]\n",
    "lead_idx_snp = df.index[df['ID'] == lead_snp_ID].tolist()[0]\n",
    "\n",
    "# 2. Calculate 't_dentist_s' and 'dentist_outlier'\n",
    "# Copied and unaltered from https://github.com/mkanai/slalom/blob/master/slalom.py\n",
    "lead_z = (df.beta / df.se).iloc[lead_idx_snp]\n",
    "df[\"t_dentist_s\"] = ((df.beta / df.se) - df.r * lead_z) ** 2 / (1 - df.r ** 2)\n",
    "df[\"t_dentist_s\"] = np.where(df[\"t_dentist_s\"] < 0, np.inf, df[\"t_dentist_s\"])\n",
    "df[\"t_dentist_s\"].iloc[lead_idx_snp] = np.nan\n",
    "df[\"nlog10p_dentist_s\"] = sp.stats.chi2.logsf(df[\"t_dentist_s\"], df=1) / -np.log(10)\n",
    "r2_threshold = 0.1\n",
    "nlog10p_dentist_s_threshold = 1e-4\n",
    "n_dentist_s_outlier = np.sum(\n",
    "            (df.R2 > r2_threshold) & (df.nlog10p_dentist_s > nlog10p_dentist_s_threshold)\n",
    "        )\n",
    "\n",
    "# Identifying outliers\n",
    "df['dentist_outlier'] = np.where((df.R2 > r2_threshold) & (df.nlog10p_dentist_s > nlog10p_dentist_s_threshold), 1, 0)\n",
    "df = df.drop(['CHR_A', 'BP_A', 'SNP_A', 'CHR_B', 'BP_B', 'SNP_B'], axis=1)\n",
    "#df.to_csv(f'{target}_locus_sumstat_with_dentist.txt.gz', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum 10\n",
      "                  ID        rsid  chromosome  position allele1 allele2  \\\n",
      "169  19:44738916:G:A   rs1531517          19  44738916       G       A   \n",
      "170  19:44744370:A:G   rs4803750          19  44744370       A       G   \n",
      "214  19:44870308:G:A    rs395908          19  44870308       G       A   \n",
      "226  19:44885917:T:A    rs283813          19  44885917       T       A   \n",
      "227  19:44886339:G:A   rs7254892          19  44886339       G       A   \n",
      "238  19:44897490:T:A  rs61679753          19  44897490       T       A   \n",
      "256  19:44910319:C:T  rs75627662          19  44910319       C       T   \n",
      "258  19:44912383:G:A    rs445925          19  44912383       G       A   \n",
      "268  19:44929300:G:C   rs7259004          19  44929300       G       C   \n",
      "273  19:44943964:G:A  rs12721109          19  44943964       G       A   \n",
      "\n",
      "         maf              p    beta      se          z        R2         r  \\\n",
      "169  0.94855  9.510000e-163  0.2202  0.0080  27.525000  0.166354  0.014799   \n",
      "170  0.94459  1.700000e-162  0.2189  0.0078  28.064103  0.180671  0.014799   \n",
      "214  0.86020   1.110000e-94  0.1568  0.0075  20.906667  0.111834  0.014799   \n",
      "226  0.93404   1.260000e-93  0.2004  0.0094  21.319149  0.123048  0.014799   \n",
      "227  0.96834   0.000000e+00  0.4853  0.0119  40.781513  0.329997  0.014799   \n",
      "238  0.96834  1.630000e-186  0.4389  0.0150  29.260000  0.345880  0.014799   \n",
      "256  0.81660   8.489000e-33  0.0813  0.0064  12.703125  0.323283  0.014799   \n",
      "258  0.91293   0.000000e+00  0.3634  0.0081  44.864198  0.697413  0.014799   \n",
      "268  0.90106  1.370000e-110  0.2094  0.0092  22.760870  0.164414  0.014799   \n",
      "273  0.98285  2.990000e-122  0.4462  0.0183  24.382514  0.162926  0.014799   \n",
      "\n",
      "     t_dentist_s  nlog10p_dentist_s  dentist_outlier  \n",
      "169   710.954448         155.906386                1  \n",
      "170   739.997261         162.221624                1  \n",
      "214   401.788831          88.648469                1  \n",
      "226   418.496964          92.285399                1  \n",
      "227  1593.741615                inf                1  \n",
      "238   806.498606         176.680846                1  \n",
      "256   140.191065          31.616568                1  \n",
      "258  1936.424676                inf                1  \n",
      "268   479.569568         105.576595                1  \n",
      "273   553.232669         121.603243                1  \n"
     ]
    }
   ],
   "source": [
    "total = df['dentist_outlier'].sum()\n",
    "print('sum',n_dentist_s_outlier)\n",
    "print(df[df['dentist_outlier'] == 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Dentist Calculation on SLALOM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/example.slalom.txt', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rsid', 'chromosome', 'position', 'allele1', 'allele2', 'maf', 'beta',\n",
       "       'se', 'p', 'all_meta_AF', 'p_het', 'n_cases', 'n_controls', 'n_samples',\n",
       "       'n_datasets', 'n_biobanks', 'is_strand_flip', 'is_diff_AF_gnomAD',\n",
       "       'n_afr', 'n_amr', 'n_eas', 'n_fin', 'n_nfe', 'in_cups', 'most_severe',\n",
       "       'gene_most_severe', 'consequence', 'gnomad_v3_af_afr',\n",
       "       'gnomad_v3_af_amr', 'gnomad_v3_af_eas', 'gnomad_v3_af_fin',\n",
       "       'gnomad_v3_af_nfe', 'gnomad_v3_af_sas', 'lbf', 'prob', 'cs', 'cs_99',\n",
       "       'lead_variant', 'gnomad_lead_r_afr', 'gnomad_lead_r_amr',\n",
       "       'gnomad_lead_r_eas', 'gnomad_lead_r_fin', 'gnomad_lead_r_nfe', 'r',\n",
       "       't_dentist_s', 'nlog10p_dentist_s'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       11.892953\n",
       "1        0.276961\n",
       "2        1.893043\n",
       "3             NaN\n",
       "4       14.072130\n",
       "          ...    \n",
       "1088     1.598529\n",
       "1089          NaN\n",
       "1090          NaN\n",
       "1091          NaN\n",
       "1092          NaN\n",
       "Name: t_dentist_s, Length: 1093, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['t_dentist_s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       11.892953\n",
       "1        0.276961\n",
       "2        1.893043\n",
       "3             NaN\n",
       "4       14.072130\n",
       "          ...    \n",
       "1088     1.598529\n",
       "1089          NaN\n",
       "1090          NaN\n",
       "1091          NaN\n",
       "1092          NaN\n",
       "Name: t_dentist_s2, Length: 1093, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['z'] = df['beta']/df['se']\n",
    "lead_z = df[(df['rsid'] == 'rs2099684')]\n",
    "lead_z_value = lead_z['z'].iloc[0]\n",
    "df['t_dentist_s2'] = (df['z'] - df['r'] * lead_z_value)**2 / (1 - df['r']**2)\n",
    "df['t_dentist_s2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CARMA Outlier detection Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import carmapy.carmapy_c\n",
    "\n",
    "sumstats = pd.read_csv(\"/Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/APOE_LDL_locus_sumstat_with_dentist.txt.gz\", sep='\\t')\n",
    "ld = pd.read_csv(\"/Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/APOE_LDL_locus_ukbb_ld.txt.gz\", sep='\\t', header=None)\n",
    "outlier_tau = 0.04\n",
    "\n",
    "index_list = np.array(sumstats.index.tolist()).astype(np.uint32)\n",
    "z = np.array(sumstats['z'].tolist()).astype(np.float64)\n",
    "ld_matrix = np.array(ld, dtype=np.float64)\n",
    "modi_ld_S = ld_matrix\n",
    "\n",
    "outlier_likelihood = carmapy.carmapy_c.outlier_Normal_fixed_sigma_marginal  # Replace with the actual function if different\n",
    "\n",
    "def ridge_fun(x, modi_ld_S, index_list, ld_matrix, z, outlier_tau, outlier_likelihood):\n",
    "    x_scalar = x[0]\n",
    "    temp_ld_S = x_scalar * modi_ld_S + (1 - x_scalar) * np.eye(modi_ld_S.shape[0])\n",
    "    ld_matrix[index_list[:, None], index_list] = temp_ld_S\n",
    "    return outlier_likelihood(index_list, ld_matrix, z, outlier_tau, len(index_list), 1)\n",
    "\n",
    "# Optimization\n",
    "initial_guess = np.array([0.5])  # Initial guess as an array\n",
    "result = minimize(\n",
    "    lambda x: -ridge_fun(x, modi_ld_S, index_list, ld_matrix, z, outlier_tau, outlier_likelihood),\n",
    "    initial_guess,  # Use the initial guess array here\n",
    "    bounds=[(0, 1)]\n",
    ")\n",
    "\n",
    "# Extract optimized parameter\n",
    "optimized_x = result.x[0]  # Extract the scalar value from the array\n",
    "\n",
    "# Modify LD matrix\n",
    "modi_ld = optimized_x * modi_ld_S + (1 - optimized_x) * np.diag(np.diag(modi_ld_S))\n",
    "\n",
    "# Calculate test_log_BF\n",
    "test_log_BF = outlier_likelihood(index_list, ld_matrix, z, outlier_tau, len(index_list), 1) - \\\n",
    "              outlier_likelihood(index_list, modi_ld, z, outlier_tau, len(index_list), 1)\n",
    "test_log_BF = -abs(test_log_BF)\n",
    "\n",
    "print('Outlier BF:', test_log_BF)\n",
    "print('This is xi hat:', optimized_x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in Progress PySpark conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/08 13:08:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import StructType, StructField, FloatType\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Finemapping Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Input variables\n",
    "gwas_file_path = \"/Users/hn9/Documents/Analysis/FM-comparison/gwas-examples/APOE-LDL/24097068-GCST002222-EFO_0004611.h.tsv.gz\"\n",
    "target = \"APOE_LDL\"\n",
    "target_chrom = 19\n",
    "target_pos = 44908822\n",
    "start_pos = target_pos - 500000\n",
    "end_pos = target_pos + 500000\n",
    "lead_snp_ID = f\"{target_chrom}:{target_pos}:C:T\"\n",
    "n_sample = 94595"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLINK v1.90b6.21 64-bit (19 Oct 2020)          www.cog-genomics.org/plink/1.9/\n",
      "(C) 2005-2020 Shaun Purcell, Christopher Chang   GNU General Public License v3\n",
      "Logging to APOE_LDL_locus_UKBB.txt.log.\n",
      "Options in effect:\n",
      "  --allow-extra-chr\n",
      "  --bfile /Users/hn9/Documents/Analysis/FM-comparison/ukb_v3_downsampled10k/ukb_v3_chr19.downsampled10k\n",
      "  --chr 19\n",
      "  --from-bp 44408822\n",
      "  --maf 0.001\n",
      "  --out APOE_LDL_locus_UKBB.txt\n",
      "  --recode A\n",
      "  --to-bp 45408822\n",
      "\n",
      "16384 MB RAM detected; reserving 8192 MB for main workspace.\n",
      "6167 out of 364540 variants loaded from .bim file.\n",
      "10000 people (0 males, 0 females, 10000 ambiguous) loaded from .fam.\n",
      "Ambiguous sex IDs written to APOE_LDL_locus_UKBB.txt.nosex .\n",
      "Using 1 thread (no multithreaded calculations invoked).\n",
      "Before main variant filters, 10000 founders and 0 nonfounders present.\n",
      "Calculating allele frequencies... 10111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989 done.\n",
      "Total genotyping rate is 0.999988.\n",
      "0 variants removed due to minor allele threshold(s)\n",
      "(--maf/--max-maf/--mac/--max-mac).\n",
      "6167 variants and 10000 people pass filters and QC.\n",
      "Note: No phenotypes present.\n",
      "--recode A to APOE_LDL_locus_UKBB.txt.raw ... 101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='plink --bfile /Users/hn9/Documents/Analysis/FM-comparison/ukb_v3_downsampled10k/ukb_v3_chr19.downsampled10k --allow-extra-chr --recode A --chr 19 --from-bp 44408822 --to-bp 45408822 --maf 0.001 --out APOE_LDL_locus_UKBB.txt', returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PLINK command (placeholder using PLINK and UKBiobank LD reference panel)\n",
    "command = f\"plink --bfile /Users/hn9/Documents/Analysis/FM-comparison/ukb_v3_downsampled10k/ukb_v3_chr{target_chrom}.downsampled10k --allow-extra-chr --recode A --chr {target_chrom} --from-bp {start_pos} --to-bp {end_pos} --maf 0.001 --out {target}_locus_UKBB.txt\"\n",
    "subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                             \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hn9/anaconda3/envs/finemap_env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/hn9/anaconda3/envs/finemap_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/hn9/anaconda3/envs/finemap_env/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinished calculating LD correlation matrix\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ld_data, ld_matrix\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m ld_data, ld_matrix \u001b[39m=\u001b[39m get_ld_matrix()\n",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m ld_data \u001b[39m=\u001b[39m ld_data\u001b[39m.\u001b[39mdrop(\u001b[39m*\u001b[39mdrop_list)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m ld_data\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     ld_data \u001b[39m=\u001b[39m ld_data\u001b[39m.\u001b[39;49mwithColumn(col, ld_data[col]\u001b[39m.\u001b[39;49mcast(\u001b[39m\"\u001b[39;49m\u001b[39mfloat\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCalculating LD correlation matrix...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m vec_assembler \u001b[39m=\u001b[39m VectorAssembler(inputCols\u001b[39m=\u001b[39mld_data\u001b[39m.\u001b[39mcolumns, outputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemap_env/lib/python3.10/site-packages/pyspark/sql/dataframe.py:5170\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5166\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5167\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_COLUMN\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   5168\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcol\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(col)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m   5169\u001b[0m     )\n\u001b[0;32m-> 5170\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mwithColumn(colName, col\u001b[39m.\u001b[39;49m_jc), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemap_env/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/envs/finemap_env/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemap_env/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/finemap_env/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_ld_matrix():\n",
    "    \"\"\"Calculates the LD matrix based on the LD data from PLINK using PySpark\"\"\"\n",
    "    # TODO debug matrix correlation in PySpark\n",
    "    # Doesn't finish running\n",
    "    ld_data = spark.read.csv(f\"{target}_locus_UKBB.txt.raw\", sep=\" \", header=True, inferSchema=True)\n",
    "    drop_list = [\"FID\", \"IID\", \"PAT\", \"MAT\", \"SEX\", \"PHENOTYPE\"]\n",
    "    ld_data = ld_data.drop(*drop_list)\n",
    "    for col in ld_data.columns:\n",
    "        ld_data = ld_data.withColumn(col, ld_data[col].cast(\"float\"))\n",
    "\n",
    "    print('Calculating LD correlation matrix...')\n",
    "\n",
    "    vec_assembler = VectorAssembler(inputCols=ld_data.columns, outputCol=\"features\")\n",
    "    ld_data_vector = vec_assembler.transform(ld_data).select(\"features\")\n",
    "    ld_matrix = Correlation.corr(ld_data_vector, \"features\").collect()[0][0]\n",
    "\n",
    "    print('Finished calculating LD correlation matrix')\n",
    "    return ld_data, ld_matrix\n",
    "\n",
    "ld_data, ld_matrix = get_ld_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/08 11:47:24 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 192.168.0.76:53025 in 10000 milliseconds\n",
      "23/10/08 11:47:37 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 13 more\n",
      "23/10/08 11:49:25 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 192.168.0.76:53025 in 10000 milliseconds\n",
      "23/10/08 11:49:50 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 192.168.0.76:53025 in 10000 milliseconds\n",
      "23/10/08 11:50:46 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 13 more\n",
      "23/10/08 11:51:24 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)\n",
      "23/10/08 11:51:45 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)\n",
      "23/10/08 11:51:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 192.168.0.76:53025 in 10000 milliseconds. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 192.168.0.76:53025 in 10000 milliseconds\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\n",
      "\t... 7 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hn9/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/hn9/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/hn9/anaconda3/envs/finemapping_env/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     ld_matrix_spark \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(ld_matrix_dict, schema\u001b[39m=\u001b[39mschema)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ld_data_spark, ld_matrix_spark\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m ld_data_spark, ld_matrix_spark \u001b[39m=\u001b[39m get_ld_matrix()\n",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ld_data_spark \u001b[39m=\u001b[39m ld_data_spark\u001b[39m.\u001b[39mdrop(\u001b[39m*\u001b[39mdrop_list)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m ld_data_spark\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     ld_data_spark \u001b[39m=\u001b[39m ld_data_spark\u001b[39m.\u001b[39;49mwithColumn(col, ld_data_spark[col]\u001b[39m.\u001b[39;49mcast(\u001b[39m\"\u001b[39;49m\u001b[39mfloat\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m column_names \u001b[39m=\u001b[39m ld_matrix\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m schema \u001b[39m=\u001b[39m StructType([StructField(name, FloatType(), \u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m column_names])\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/dataframe.py:5170\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5166\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5167\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_COLUMN\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   5168\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcol\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(col)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m   5169\u001b[0m     )\n\u001b[0;32m-> 5170\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mwithColumn(colName, col\u001b[39m.\u001b[39;49m_jc), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_ld_matrix():\n",
    "    # Calculate LD correlation\n",
    "    # TODO debug matrix correlation in PySpark\n",
    "    # Doesn't finish running\n",
    "    ld_data = pd.read_csv(f\"{target}_locus_UKBB.txt.raw\", delim_whitespace=True)\n",
    "    ld_data = ld_data.drop(columns=[\"FID\", \"IID\", \"PAT\", \"MAT\", \"SEX\", \"PHENOTYPE\"])\n",
    "    ld_matrix = ld_data.corr(method='pearson')\n",
    "    ld_data_spark = spark.read.csv(f\"{target}_locus_UKBB.txt.raw\", sep=\" \", header=True, inferSchema=True)\n",
    "    drop_list = [\"FID\", \"IID\", \"PAT\", \"MAT\", \"SEX\", \"PHENOTYPE\"]\n",
    "    ld_data_spark = ld_data_spark.drop(*drop_list)\n",
    "    for col in ld_data_spark.columns:\n",
    "        ld_data_spark = ld_data_spark.withColumn(col, ld_data_spark[col].cast(\"float\"))\n",
    "    column_names = ld_matrix.columns.tolist()\n",
    "    schema = StructType([StructField(name, FloatType(), True) for name in column_names])\n",
    "    ld_matrix_dict = ld_matrix.reset_index(drop=True).to_dict('records')\n",
    "    ld_matrix_spark = spark.createDataFrame(ld_matrix_dict, schema=schema)\n",
    "    return ld_data_spark, ld_matrix_spark\n",
    "ld_data_spark, ld_matrix_spark = get_ld_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ld_matrix():\n",
    "    # Calculate LD correlation\n",
    "    ld_data = pd.read_csv(f\"{target}_locus_UKBB.txt.raw\", delim_whitespace=True)\n",
    "    ld_data = ld_data.drop(columns=[\"FID\", \"IID\", \"PAT\", \"MAT\", \"SEX\", \"PHENOTYPE\"])\n",
    "    ld_matrix = ld_data.corr(method='pearson')\n",
    "    return ld_data, ld_matrix\n",
    "ld_data, ld_matrix = get_ld_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ld_data_spark = spark.read.csv(f\"{target}_locus_UKBB.txt.raw\", sep=\" \", header=True, inferSchema=True)\n",
    "drop_list = [\"FID\", \"IID\", \"PAT\", \"MAT\", \"SEX\", \"PHENOTYPE\"]\n",
    "ld_data_spark = ld_data_spark.drop(*drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hn9/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/08 13:31:32 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 165043 ms exceeds timeout 120000 ms\n",
      "23/10/08 13:31:32 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o101.legacyInferArrayTypeFromFirstElement. Trace:\npy4j.Py4JException: Method legacyInferArrayTypeFromFirstElement([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ld_matrix_spark \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(ld_matrix)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data, columns\u001b[39m=\u001b[39mcolumn_names)\n\u001b[1;32m   1438\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m-> 1440\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(SparkSession, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[1;32m   1443\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:363\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    362\u001b[0m converted_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[1;32m   1486\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m-> 1093\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchemaFromList(data, names\u001b[39m=\u001b[39;49mschema)\n\u001b[1;32m   1094\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:953\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    949\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCANNOT_INFER_EMPTY_SCHEMA\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    950\u001b[0m         message_parameters\u001b[39m=\u001b[39m{},\n\u001b[1;32m    951\u001b[0m     )\n\u001b[1;32m    952\u001b[0m infer_dict_as_struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jconf\u001b[39m.\u001b[39minferDictAsStruct()\n\u001b[0;32m--> 953\u001b[0m infer_array_from_first_element \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jconf\u001b[39m.\u001b[39;49mlegacyInferArrayTypeFromFirstElement()\n\u001b[1;32m    954\u001b[0m prefer_timestamp_ntz \u001b[39m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m    955\u001b[0m schema \u001b[39m=\u001b[39m reduce(\n\u001b[1;32m    956\u001b[0m     _merge_type,\n\u001b[1;32m    957\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m     ),\n\u001b[1;32m    967\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o101.legacyInferArrayTypeFromFirstElement. Trace:\npy4j.Py4JException: Method legacyInferArrayTypeFromFirstElement([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"
     ]
    }
   ],
   "source": [
    "ld_matrix_spark = spark.createDataFrame(ld_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m schema \u001b[39m=\u001b[39m StructType([StructField(name, FloatType(), \u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m column_names])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ld_matrix_dict \u001b[39m=\u001b[39m ld_matrix\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto_dict(\u001b[39m'\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ld_matrix_spark \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(ld_matrix_dict, schema\u001b[39m=\u001b[39;49mschema)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(SparkSession, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:1487\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1485\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromLocal(\u001b[39mmap\u001b[39m(prepare, data), schema)\n\u001b[1;32m   1486\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1487\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39;49m_to_java_object_rdd())\n\u001b[1;32m   1488\u001b[0m jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsparkSession\u001b[39m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[39m.\u001b[39mrdd(), struct\u001b[39m.\u001b[39mjson())\n\u001b[1;32m   1489\u001b[0m df \u001b[39m=\u001b[39m DataFrame(jdf, \u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/rdd.py:4918\u001b[0m, in \u001b[0;36mRDD._to_java_object_rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4915\u001b[0m rdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pickled()\n\u001b[1;32m   4916\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 4918\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mpythonToJava(rdd\u001b[39m.\u001b[39;49m_jrdd, \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/rdd.py:5470\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5467\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5468\u001b[0m     profiler \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 5470\u001b[0m wrapped_func \u001b[39m=\u001b[39m _wrap_function(\n\u001b[1;32m   5471\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prev_jrdd_deserializer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd_deserializer, profiler\n\u001b[1;32m   5472\u001b[0m )\n\u001b[1;32m   5474\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   5475\u001b[0m python_rdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD(\n\u001b[1;32m   5476\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prev_jrdd\u001b[39m.\u001b[39mrdd(), wrapped_func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreservesPartitioning, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_barrier\n\u001b[1;32m   5477\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/rdd.py:5270\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5268\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[39m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[1;32m   5269\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 5270\u001b[0m \u001b[39mreturn\u001b[39;00m sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mSimplePythonFunction(\n\u001b[1;32m   5271\u001b[0m     \u001b[39mbytearray\u001b[39;49m(pickled_command),\n\u001b[1;32m   5272\u001b[0m     env,\n\u001b[1;32m   5273\u001b[0m     includes,\n\u001b[1;32m   5274\u001b[0m     sc\u001b[39m.\u001b[39;49mpythonExec,\n\u001b[1;32m   5275\u001b[0m     sc\u001b[39m.\u001b[39;49mpythonVer,\n\u001b[1;32m   5276\u001b[0m     broadcast_vars,\n\u001b[1;32m   5277\u001b[0m     sc\u001b[39m.\u001b[39;49m_javaAccumulator,\n\u001b[1;32m   5278\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "column_names = ld_matrix.columns.tolist()\n",
    "schema = StructType([StructField(name, FloatType(), True) for name in column_names])\n",
    "ld_matrix_dict = ld_matrix.reset_index(drop=True).to_dict('records')\n",
    "ld_matrix_spark = spark.createDataFrame(ld_matrix_dict, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in as hail and then do pyspark conversion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def get_sumstats(gwas_file_path, target_chrom, start_pos, end_pos):\n",
    "    sumstat = spark.read.csv(gwas_file_path, header=True, sep=\"\\t\")\n",
    "    sumstat = sumstat.filter(\n",
    "        (col('hm_chrom') == target_chrom) & \n",
    "        (col('hm_pos') >= start_pos) & \n",
    "        (col('hm_pos') <= end_pos)\n",
    "    )\n",
    "    sumstat = sumstat.dropna(subset=['hm_chrom'])\n",
    "\n",
    "    sumstat = sumstat.withColumn('z', col('beta') / col('standard_error'))\n",
    "\n",
    "    cols_to_rename = {\n",
    "        'hm_variant_id': 'ID',\n",
    "        'hm_rsid': 'rsid',\n",
    "        #'hm_chrom': 'chromosome', - chromosome col already exists\n",
    "        'hm_pos': 'position',\n",
    "        'hm_other_allele': 'allele1',\n",
    "        'hm_effect_allele': 'allele2',\n",
    "        'hm_effect_allele_frequency': 'maf',\n",
    "        'standard_error': 'se',\n",
    "        'p_value': 'p'\n",
    "    }\n",
    "    for old_name, new_name in cols_to_rename.items():\n",
    "        sumstat = sumstat.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    selected_cols = ['ID', 'rsid', 'chromosome', 'position', 'allele1', 'allele2', 'maf', 'p', 'beta', 'se', 'z']\n",
    "    sumstat = sumstat.select(selected_cols)\n",
    "    \n",
    "    return sumstat\n",
    "sumstat  = get_sumstats(gwas_file_path, target_chrom, start_pos, end_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_snps(sumstat, ld_matrix_spark, ld_data_spark):\n",
    "    \"\"\"Matches SNPs between summary statistics and LD matrix and filters them accordingly.\"\"\"\n",
    "    # Regex pattern for extracting position from SNP\n",
    "    pattern = re.compile(r\"(^\\d+)|(?<=:)\\d+(?=:|$)\")\n",
    "    ld_data_spark = ld_data_spark.withColumn(\"position\", F.regexp_extract(F.col(\"SNP\"), pattern, 0))\n",
    "    ld_data_spark = ld_data_spark.withColumn(\"ID\", F.regexp_replace(F.col(\"SNP\"), r'[:,_]', '_'))\n",
    "    \n",
    "    # Join sumstat with ld_data on 'ID'\n",
    "    concordance_test = sumstat.join(ld_data_spark, 'ID', 'inner')\n",
    "    \n",
    "    # Filter sumstat and ld_matrix for ID matches only\n",
    "    sumstat_filtered = sumstat.filter(F.col(\"ID\").isin(concordance_test.select(\"ID\").distinct().rdd.flatMap(lambda x: x).collect()))\n",
    "    ld_matrix_filtered = ld_matrix_spark.filter(F.col(\"ID\").isin(concordance_test.select(\"ID\").distinct().rdd.flatMap(lambda x: x).collect()))\n",
    "\n",
    "    return sumstat_filtered, ld_matrix_filtered, concordance_test\n",
    "\n",
    "sumstat_filtered, ld_matrix_filtered, concordance_test = match_snps(sumstat, ld_matrix, ld_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_snps(sumstat, ld_matrix, ld_data):\n",
    "    \"\"\"Matches SNPs between summary statistics and LD matrix and filters them accordingly.\"\"\"\n",
    "    # Getting only SNPs in sumstats that are also in the LD matrix\n",
    "    # Get SNP IDs from ld matrix to compare with sumstat SNP IDs\n",
    "    pattern = re.compile(r\"(^\\d+)|(?<=:)\\d+(?=:|$)\")\n",
    "    df1_transpose = ld_data.T.reset_index()\n",
    "    df1_transpose.columns = ['SNP'] + list(df1_transpose.columns[1:])\n",
    "    df1_transpose['position'] = df1_transpose['SNP'].apply(lambda x: re.search(pattern, x).group())\n",
    "    df1_transpose['ID'] = df1_transpose['SNP'].str.replace(r'[:,_]', '_').str.replace(r'_[^_]+$', '')\n",
    "    concordance_test = pd.merge(sumstat, df1_transpose, on='ID')\n",
    "\n",
    "    # Filter sumstat and LD matrix for ID matches only\n",
    "    sumstat_filtered = sumstat[sumstat['ID'].isin(concordance_test['ID'])]\n",
    "    sumstat_filtered.reset_index(drop=True, inplace=True)\n",
    "    ld_matrix_filtered = ld_matrix.loc[concordance_test['SNP'], concordance_test['SNP']]\n",
    "    return sumstat_filtered, ld_matrix_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allele_flip_check(concordance_test, sumstat_filtered, ld_matrix_filtered):\n",
    "    # Extract alleles using PySpark string functions\n",
    "    allele_df = concordance_test.withColumn('allele_parts', F.split('SNP', '[:,_]'))\n",
    "    concordance_test = allele_df.withColumn('allele1_LD', allele_df['allele_parts'].getItem(1)).\\\n",
    "                                 withColumn('allele2_LD', allele_df['allele_parts'].getItem(2))\n",
    "\n",
    "    # Join sumstat_filtered and concordance_test to align them\n",
    "    joint_df = sumstat_filtered.join(concordance_test, 'ID', 'inner')\n",
    "\n",
    "    # Flip z-scores if alleles are discordant\n",
    "    condition = (joint_df['allele1'] != joint_df['allele1_LD']) | (joint_df['allele2'] != joint_df['allele2_LD'])\n",
    "    sumstat_filtered = joint_df.withColumn('z', F.when(condition, -joint_df['z']).otherwise(joint_df['z']))\n",
    "\n",
    "    # Check for NaNs or Inf values in ld_matrix_filtered and sumstat_filtered\n",
    "    # Here it depends on your ld_matrix_filtered structure. Assuming it's a PySpark DataFrame:\n",
    "    any_na_matrix = ld_matrix_filtered.filter(F.isnan(ld_matrix_filtered.any_column())).count() > 0  # Replace any_column with actual column name(s)\n",
    "    any_na_sumstat = sumstat_filtered.filter(F.isnan(sumstat_filtered['z'])).count() > 0\n",
    "\n",
    "    print(f\"Correlation matrix has NaNs: {any_na_matrix}\")\n",
    "    print(f\"Sumstat has NaNs: {any_na_sumstat}\")\n",
    "\n",
    "    # Saving DataFrames to files\n",
    "    ld_matrix_filtered.write.csv(f\"{target}_locus_ukbb_ld.txt.gz\", sep='\\t', header=False)\n",
    "    sumstat_filtered.write.csv(f\"{target}_locus_sumstat_flip_check.txt.gz\", sep='\\t', header=True)\n",
    "\n",
    "    return sumstat_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dentist_calc(sumstat, target, lead_snp_ID, n_sample):\n",
    "    # 1. Getting R2 column for sumstats\n",
    "    ld = spark_session.read.csv(f'/Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/ld/{target}_subset_for_ld_calculation.ld', sep='\\\\s+', header=True)\n",
    "    lead_ld = ld.filter((ld['SNP_A'] == lead_snp_ID) | (ld['SNP_B'] == lead_snp_ID))\n",
    "    sumstat = spark_session.read.csv(f'/Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/{target}_locus_sumstat_flip_check.txt.gz', sep='\\t', header=True)\n",
    "    sumstat = sumstat.withColumn('ID', F.regexp_replace('ID', r\"_(\\d+)_([A-Z])_([A-Z])\", r\":\\1:\\2:\\3\"))\n",
    "    merged = lead_ld.join(sumstat.select('ID'), lead_ld['SNP_B'] == sumstat['ID'])\n",
    "    df = merged.select('ID', 'R2')\n",
    "    df = df.join(sumstat, 'ID', 'left_outer')\n",
    "    r_value = (n_sample * df.agg(F.sum('R2')).first()[0]) / (n_sample * df.agg(F.count('R2')).first()[0])\n",
    "    df = df.withColumn('r', F.lit(r_value))\n",
    "\n",
    "    lead_row = df.filter(df['ID'] == lead_snp_ID).first()\n",
    "    lead_z = lead_row['beta'] / lead_row['se']\n",
    "\n",
    "    # 2. Calculate 't_dentist_s' and 'dentist_outlier'\n",
    "    df = df.withColumn('t_dentist_s', (df['z'] - df['r'] * lead_z)**2 / (1 - df['r']**2))\n",
    "    df = df.withColumn('dentist_outlier', F.when((df['t_dentist_s'] < 1e-4) & (df['R2'] > 0.6), 1).otherwise(0))\n",
    "    df = df.drop('CHR_A', 'BP_A', 'SNP_A', 'CHR_B', 'BP_B', 'SNP_B')\n",
    "    df.write.csv(f'{target}_locus_sumstat_with_dentist.txt.gz', sep='\\t', header=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_data, ld_matrix = get_ld_matrix()\n",
    "sumstat = get_sumstats(gwas_file_path)\n",
    "sumstat_filtered, ld_matrix_filtered, concordance_test = match_snps(sumstat, ld_matrix, ld_data)\n",
    "sumstat_filtered = allele_flip_check(concordance_test, sumstat_filtered)\n",
    "\n",
    "lead_ld_command = f\"\"\"plink --bfile /Users/hn9/Documents/Analysis/FM-comparison/ukb_v3_downsampled10k/ukb_v3_chr{target_chrom}.downsampled10k \\\n",
    "        --allow-extra-chr \\\n",
    "        --r2 \\\n",
    "        --ld-snp {lead_snp_ID1} \\\n",
    "        --ld-window-kb 1000 \\\n",
    "        --ld-window 99999 \\\n",
    "        --ld-window-r2 0 \\\n",
    "        --out /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/ld/{target}_subset_for_ld_calculation\n",
    "\"\"\"\n",
    "subprocess.run(lead_ld_command, shell=True)\n",
    "\n",
    "df = dentist_calc(sumstat_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Reading summary statistics from file /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/APOE_LDL_locus_sumstat_with_dentist.txt.gz\n",
      "481 SNPs in summary statistics file\n",
      "Reading in LD matrix from file /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/APOE_LDL_locus_ukbb_ld.txt.gz\n",
      "Reading in LD matrix took 0.05 seconds\n",
      "Performing eigen decomposition\n",
      "Eigen decomposition took 0.06 seconds\n",
      "Running susieinf\n",
      "*********************************************************************\n",
      "* SuSiE-inf\n",
      "* Version 1.3\n",
      "* (C) Ran Cui, Zhou Fan\n",
      "*********************************************************************\n",
      "Iteration 0\n",
      "Update s^2 for effect 0 to 0.000006\n",
      "Update s^2 for effect 1 to 0.000006\n",
      "Update s^2 for effect 2 to 0.000006\n",
      "Update s^2 for effect 3 to 0.000006\n",
      "Update s^2 for effect 4 to 0.000006\n",
      "Update s^2 for effect 5 to 0.000006\n",
      "Update s^2 for effect 6 to 0.000006\n",
      "Update s^2 for effect 7 to 0.000006\n",
      "Update s^2 for effect 8 to 0.000006\n",
      "Update s^2 for effect 9 to 0.000006\n",
      "Update (sigma^2,tau^2) to (1.000038,0.000000e+00)\n",
      "Maximum change in PIP: 0.000209\n",
      "CONVERGED\n",
      "Running SuSiE-inf took 0.05 seconds\n",
      "Saving output to APOE_LDL_locus.susieinf.bgz\n",
      "Total time elapsed 0.30 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python /Users/hn9/Documents/GitHub/fine-mapping-inf/run_fine_mapping.py     --sumstats /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/APOE_LDL_locus_sumstat_with_dentist.txt.gz     --beta-col-name beta     --se-col-name se     --ld-file /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/APOE_LDL_locus_ukbb_ld.txt.gz     --n 94595     --method susieinf     --save-tsv     --eigen-decomp-prefix APOE_LDL_locus     --output-prefix  APOE_LDL_locus ', returncode=0)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "susieinf_command = f\"\"\"python /Users/hn9/Documents/GitHub/fine-mapping-inf/run_fine_mapping.py \\\n",
    "    --sumstats /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/{target}_locus_sumstat_with_dentist.txt.gz \\\n",
    "    --beta-col-name beta \\\n",
    "    --se-col-name se \\\n",
    "    --ld-file /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/{target}_locus_ukbb_ld.txt.gz \\\n",
    "    --n {n_sample} \\\n",
    "    --method susieinf \\\n",
    "    --save-tsv \\\n",
    "    --eigen-decomp-prefix {target}_locus \\\n",
    "    --output-prefix  {target}_locus \"\"\"\n",
    "\n",
    "subprocess.run(susieinf_command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='gunzip -c APOE_LDL_locus.susieinf.bgz > APOE_LDL_locus.txt', returncode=0)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_comamnd = f\"\"\"gunzip -c {target}_locus.susieinf.bgz > {target}_locus.txt\"\"\"\n",
    "subprocess.run(zip_comamnd, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in Progress PySpark conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/10 11:12:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import StructType, StructField, FloatType\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "import hail as hl\n",
    "from hail.linalg import BlockMatrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Finemapping Pipeline\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\",True) \\\n",
    "    .config(\"spark.memory.offHeap.size\",\"16g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Input variables\n",
    "gwas_file_path = \"/Users/hn9/Documents/Analysis/FM-comparison/gwas-examples/APOE-LDL/24097068-GCST002222-EFO_0004611.h.tsv.gz\"\n",
    "target = \"APOE_LDL\"\n",
    "target_chrom = 19\n",
    "target_pos = 44908822\n",
    "start_pos = target_pos - 500000\n",
    "end_pos = target_pos + 500000\n",
    "lead_snp_ID = f\"{target_chrom}:{target_pos}:C:T\"\n",
    "n_sample = 94595"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift_and_calculate_ld_gnomad(lead_snp_ID, StudyLocus):\n",
    "    bm = BlockMatrix.read(\"gs://gcp-public-data--gnomad/release/2.1.1/ld/gnomad.genomes.r2.1.1.nfe.common.adj.ld.bm\")\n",
    "    variant_table = hl.read_table(\"gs://gcp-public-data--gnomad/release/2.1.1/ld/gnomad.genomes.r2.1.1.nfe.common.adj.ld.variant_indices.ht\")\n",
    "    locus = hl.parse_variant(lead_snp_ID).locus\n",
    "    temp_table = hl.Table.parallelize([hl.struct(locus=locus)])\n",
    "    locus_values = temp_table.select('locus').collect()\n",
    "    locus_value = locus_values[0].locus\n",
    "    contig = locus_value.contig\n",
    "    position = locus_value.position\n",
    "    locus_37 = hl.liftover(hl.locus(contig, position, 'GRCh38'), 'GRCh37')\n",
    "\n",
    "    window_size = 500000\n",
    "    locus_variants = variant_table.filter(\n",
    "        (hl.abs(variant_table.locus.position - locus_37.position) <= window_size) &\n",
    "        (variant_table.locus.contig == locus_37.contig)\n",
    "    )\n",
    "\n",
    "    # Get LD matrix\n",
    "    indices = locus_variants['idx'].collect()\n",
    "    sub_bm = bm.filter(indices, indices)\n",
    "    numpy_array = sub_bm.to_numpy()\n",
    "    # need to change to iteritems due to old pandas version error\n",
    "    ld_df.iteritems = ld_df.items\n",
    "    ld_df = pd.DataFrame(numpy_array)\n",
    "    ld_pyspark_df = spark.createDataFrame(ld_df)\n",
    "\n",
    "    # Get SNP IDs in GRCh38\n",
    "    locus_variants = locus_variants.annotate(\n",
    "        locus_38 = hl.liftover(locus_variants.locus, 'GRCh38')\n",
    "    )\n",
    "    locus_variants = locus_variants.annotate(\n",
    "        snp_id_38 = hl.str(locus_variants.locus_38.contig) + \"_\" +\n",
    "                    hl.str(locus_variants.locus_38.position) + \"_\" +\n",
    "                    locus_variants.alleles[0] + \"_\" +\n",
    "                    locus_variants.alleles[1]\n",
    "    )\n",
    "    snp_ids_38 = locus_variants['snp_id_38'].collect()\n",
    "\n",
    "    study_snps = [row.snp_id_38 for row in StudyLocus.select(\"variantID\").collect()]\n",
    "    study_snps = set(study_snps)\n",
    "    # Find the intersection of the SNPs\n",
    "    common_snps = study_snps.intersection(snp_ids_38)\n",
    "    \n",
    "    # Filter StudyLocus to only include common SNPs\n",
    "    filtered_StudyLocus = StudyLocus.filter(F.col(\"snp_id_38\").isin(common_snps))\n",
    "    \n",
    "    # Filter the LD matrix to include only the common SNPs\n",
    "    # First, we'll create a DataFrame with the correct column names\n",
    "    ld_df.columns = snp_ids_38\n",
    "    ld_pyspark_df = spark.createDataFrame(ld_df)\n",
    "    \n",
    "    # Next, we'll filter the DataFrame to only include the common SNPs\n",
    "    selected_columns = [col for col in ld_pyspark_df.columns if col in common_snps]\n",
    "    filtered_ld_pyspark_df = ld_pyspark_df.select(selected_columns)\n",
    "    \n",
    "    return filtered_ld_pyspark_df, filtered_StudyLocus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Hail with default parameters...\n",
      "/Users/hn9/anaconda3/envs/test_env/lib/python3.8/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:43: UserWarning: Reading spark-defaults.conf to determine GCS requester pays configuration. This is deprecated. Please use `hailctl config set gcs_requester_pays/project` and `hailctl config set gcs_requester_pays/buckets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 4\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lift_and_calculate_ld_gnomad(\u001b[39m\"\u001b[39;49m\u001b[39mchr8:27610986:C:A\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 4\u001b[0m line \u001b[0;36mlift_and_calculate_ld_gnomad\u001b[0;34m(lead_snp_ID)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlift_and_calculate_ld_gnomad\u001b[39m(lead_snp_ID):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     bm \u001b[39m=\u001b[39m BlockMatrix\u001b[39m.\u001b[39mread(\u001b[39m\"\u001b[39m\u001b[39mgs://gcp-public-data--gnomad/release/2.1.1/ld/gnomad.genomes.r2.1.1.nfe.common.adj.ld.bm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     variant_table \u001b[39m=\u001b[39m hl\u001b[39m.\u001b[39;49mread_table(\u001b[39m\"\u001b[39;49m\u001b[39mgs://gcp-public-data--gnomad/release/2.1.1/ld/gnomad.genomes.r2.1.1.nfe.common.adj.ld.variant_indices.ht\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     locus \u001b[39m=\u001b[39m hl\u001b[39m.\u001b[39mparse_variant(lead_snp_ID)\u001b[39m.\u001b[39mlocus\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     temp_table \u001b[39m=\u001b[39m hl\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mparallelize([hl\u001b[39m.\u001b[39mstruct(locus\u001b[39m=\u001b[39mlocus)])\n",
      "File \u001b[0;32m~/anaconda3/envs/test_env/lib/python3.8/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/test_env/lib/python3.8/site-packages/hail/typecheck/check.py:584\u001b[0m, in \u001b[0;36m_make_dec.<locals>.wrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[39m@decorator\u001b[39m\n\u001b[1;32m    582\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(__original_func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    583\u001b[0m     args_, kwargs_ \u001b[39m=\u001b[39m check_all(__original_func, args, kwargs, checkers, is_method\u001b[39m=\u001b[39mis_method)\n\u001b[0;32m--> 584\u001b[0m     \u001b[39mreturn\u001b[39;00m __original_func(\u001b[39m*\u001b[39;49margs_, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_)\n",
      "File \u001b[0;32m~/anaconda3/envs/test_env/lib/python3.8/site-packages/hail/methods/impex.py:3059\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(path, _intervals, _filter_intervals, _n_partitions, _assert_type, _load_refs, _create_row_uids)\u001b[0m\n\u001b[1;32m   3047\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Read in a :class:`.Table` written with :meth:`.Table.write`.\u001b[39;00m\n\u001b[1;32m   3048\u001b[0m \n\u001b[1;32m   3049\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3056\u001b[0m \u001b[39m:class:`.Table`\u001b[39;00m\n\u001b[1;32m   3057\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3058\u001b[0m \u001b[39mif\u001b[39;00m _load_refs:\n\u001b[0;32m-> 3059\u001b[0m     \u001b[39mfor\u001b[39;00m rg_config \u001b[39min\u001b[39;00m Env\u001b[39m.\u001b[39;49mbackend()\u001b[39m.\u001b[39mload_references_from_dataset(path):\n\u001b[1;32m   3060\u001b[0m         hl\u001b[39m.\u001b[39mReferenceGenome\u001b[39m.\u001b[39m_from_config(rg_config)\n\u001b[1;32m   3062\u001b[0m \u001b[39mif\u001b[39;00m _intervals \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m _n_partitions \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/test_env/lib/python3.8/site-packages/hail/utils/java.py:88\u001b[0m, in \u001b[0;36mEnv.backend\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackend\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhail.backend.Backend\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m Env\u001b[39m.\u001b[39;49mhc()\u001b[39m.\u001b[39m_backend\n",
      "File \u001b[0;32m~/anaconda3/envs/test_env/lib/python3.8/site-packages/hail/utils/java.py:66\u001b[0m, in \u001b[0;36mEnv.hc\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m     sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mflush()\n\u001b[1;32m     65\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontext\u001b[39;00m \u001b[39mimport\u001b[39;00m init\n\u001b[0;32m---> 66\u001b[0m     init()\n\u001b[1;32m     68\u001b[0m \u001b[39massert\u001b[39;00m Env\u001b[39m.\u001b[39m_hc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m Env\u001b[39m.\u001b[39m_hc\n",
      "File \u001b[0;32m~/anaconda3/envs/test_env/lib/python3.8/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/test_env/lib/python3.8/site-packages/hail/typecheck/check.py:584\u001b[0m, in \u001b[0;36m_make_dec.<locals>.wrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[39m@decorator\u001b[39m\n\u001b[1;32m    582\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(__original_func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    583\u001b[0m     args_, kwargs_ \u001b[39m=\u001b[39m check_all(__original_func, args, kwargs, checkers, is_method\u001b[39m=\u001b[39mis_method)\n\u001b[0;32m--> 584\u001b[0m     \u001b[39mreturn\u001b[39;00m __original_func(\u001b[39m*\u001b[39;49margs_, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_)\n",
      "File \u001b[0;32m~/anaconda3/envs/test_env/lib/python3.8/site-packages/hail/context.py:353\u001b[0m, in \u001b[0;36minit\u001b[0;34m(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, backend, driver_cores, driver_memory, worker_cores, worker_memory, gcs_requester_pays_configuration, regions)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[39mreturn\u001b[39;00m asyncio\u001b[39m.\u001b[39mget_event_loop()\u001b[39m.\u001b[39mrun_until_complete(init_batch(\n\u001b[1;32m    337\u001b[0m         log\u001b[39m=\u001b[39mlog,\n\u001b[1;32m    338\u001b[0m         quiet\u001b[39m=\u001b[39mquiet,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         regions\u001b[39m=\u001b[39mregions\n\u001b[1;32m    351\u001b[0m     ))\n\u001b[1;32m    352\u001b[0m \u001b[39mif\u001b[39;00m backend \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mspark\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m init_spark(\n\u001b[1;32m    354\u001b[0m         sc\u001b[39m=\u001b[39;49msc,\n\u001b[1;32m    355\u001b[0m         app_name\u001b[39m=\u001b[39;49mapp_name,\n\u001b[1;32m    356\u001b[0m         master\u001b[39m=\u001b[39;49mmaster,\n\u001b[1;32m    357\u001b[0m         local\u001b[39m=\u001b[39;49mlocal,\n\u001b[1;32m    358\u001b[0m         min_block_size\u001b[39m=\u001b[39;49mmin_block_size,\n\u001b[1;32m    359\u001b[0m         branching_factor\u001b[39m=\u001b[39;49mbranching_factor,\n\u001b[1;32m    360\u001b[0m         spark_conf\u001b[39m=\u001b[39;49mspark_conf,\n\u001b[1;32m    361\u001b[0m         _optimizer_iterations\u001b[39m=\u001b[39;49m_optimizer_iterations,\n\u001b[1;32m    362\u001b[0m         log\u001b[39m=\u001b[39;49mlog,\n\u001b[1;32m    363\u001b[0m         quiet\u001b[39m=\u001b[39;49mquiet,\n\u001b[1;32m    364\u001b[0m         append\u001b[39m=\u001b[39;49mappend,\n\u001b[1;32m    365\u001b[0m         tmp_dir\u001b[39m=\u001b[39;49mtmp_dir,\n\u001b[1;32m    366\u001b[0m         local_tmpdir\u001b[39m=\u001b[39;49mlocal_tmpdir,\n\u001b[1;32m    367\u001b[0m         default_reference\u001b[39m=\u001b[39;49mdefault_reference,\n\u001b[1;32m    368\u001b[0m         global_seed\u001b[39m=\u001b[39;49mglobal_seed,\n\u001b[1;32m    369\u001b[0m         skip_logging_configuration\u001b[39m=\u001b[39;49mskip_logging_configuration,\n\u001b[1;32m    370\u001b[0m         gcs_requester_pays_configuration\u001b[39m=\u001b[39;49mgcs_requester_pays_configuration\n\u001b[1;32m    371\u001b[0m     )\n\u001b[1;32m    372\u001b[0m \u001b[39mif\u001b[39;00m backend \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m init_local(\n\u001b[1;32m    374\u001b[0m         log\u001b[39m=\u001b[39mlog,\n\u001b[1;32m    375\u001b[0m         quiet\u001b[39m=\u001b[39mquiet,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m         gcs_requester_pays_configuration\u001b[39m=\u001b[39mgcs_requester_pays_configuration\n\u001b[1;32m    382\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/test_env/lib/python3.8/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/test_env/lib/python3.8/site-packages/hail/typecheck/check.py:584\u001b[0m, in \u001b[0;36m_make_dec.<locals>.wrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[39m@decorator\u001b[39m\n\u001b[1;32m    582\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(__original_func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    583\u001b[0m     args_, kwargs_ \u001b[39m=\u001b[39m check_all(__original_func, args, kwargs, checkers, is_method\u001b[39m=\u001b[39mis_method)\n\u001b[0;32m--> 584\u001b[0m     \u001b[39mreturn\u001b[39;00m __original_func(\u001b[39m*\u001b[39;49margs_, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_)\n",
      "File \u001b[0;32m~/anaconda3/envs/test_env/lib/python3.8/site-packages/hail/context.py:436\u001b[0m, in \u001b[0;36minit_spark\u001b[0;34m(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, gcs_requester_pays_configuration)\u001b[0m\n\u001b[1;32m    430\u001b[0m app_name \u001b[39m=\u001b[39m app_name \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mHail\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    431\u001b[0m gcs_requester_pays_project, gcs_requester_pays_buckets \u001b[39m=\u001b[39m convert_gcs_requester_pays_configuration_to_hadoop_conf_style(\n\u001b[1;32m    432\u001b[0m     get_gcs_requester_pays_configuration(\n\u001b[1;32m    433\u001b[0m         gcs_requester_pays_configuration\u001b[39m=\u001b[39mgcs_requester_pays_configuration,\n\u001b[1;32m    434\u001b[0m     )\n\u001b[1;32m    435\u001b[0m )\n\u001b[0;32m--> 436\u001b[0m backend \u001b[39m=\u001b[39m SparkBackend(\n\u001b[1;32m    437\u001b[0m     idempotent, sc, spark_conf, app_name, master, local, log,\n\u001b[1;32m    438\u001b[0m     quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir,\n\u001b[1;32m    439\u001b[0m     skip_logging_configuration, optimizer_iterations,\n\u001b[1;32m    440\u001b[0m     gcs_requester_pays_project\u001b[39m=\u001b[39;49mgcs_requester_pays_project,\n\u001b[1;32m    441\u001b[0m     gcs_requester_pays_buckets\u001b[39m=\u001b[39;49mgcs_requester_pays_buckets\n\u001b[1;32m    442\u001b[0m )\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m backend\u001b[39m.\u001b[39mfs\u001b[39m.\u001b[39mexists(tmpdir):\n\u001b[1;32m    444\u001b[0m     backend\u001b[39m.\u001b[39mfs\u001b[39m.\u001b[39mmkdir(tmpdir)\n",
      "File \u001b[0;32m~/anaconda3/envs/test_env/lib/python3.8/site-packages/hail/backend/spark_backend.py:208\u001b[0m, in \u001b[0;36mSparkBackend.__init__\u001b[0;34m(self, idempotent, sc, spark_conf, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir, skip_logging_configuration, optimizer_iterations, gcs_requester_pays_project, gcs_requester_pays_buckets)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jhc \u001b[39m=\u001b[39m hail_package\u001b[39m.\u001b[39mHailContext\u001b[39m.\u001b[39mgetOrCreate(\n\u001b[1;32m    206\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jbackend, branching_factor, optimizer_iterations)\n\u001b[1;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jbackend \u001b[39m=\u001b[39m hail_package\u001b[39m.\u001b[39;49mbackend\u001b[39m.\u001b[39;49mspark\u001b[39m.\u001b[39;49mSparkBackend\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    209\u001b[0m         jsc, app_name, master, local, log, \u001b[39mTrue\u001b[39;49;00m, append, skip_logging_configuration, min_block_size, tmpdir, local_tmpdir,\n\u001b[1;32m    210\u001b[0m         gcs_requester_pays_project, gcs_requester_pays_buckets)\n\u001b[1;32m    211\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jhc \u001b[39m=\u001b[39m hail_package\u001b[39m.\u001b[39mHailContext\u001b[39m.\u001b[39mapply(\n\u001b[1;32m    212\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jbackend, branching_factor, optimizer_iterations)\n\u001b[1;32m    214\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jbackend\u001b[39m.\u001b[39msc()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "lift_and_calculate_ld_gnomad(\"chr8:27610986:C:A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLINK v1.90b6.21 64-bit (19 Oct 2020)          www.cog-genomics.org/plink/1.9/\n",
      "(C) 2005-2020 Shaun Purcell, Christopher Chang   GNU General Public License v3\n",
      "Logging to APOE_LDL_locus_UKBB.txt.log.\n",
      "Options in effect:\n",
      "  --allow-extra-chr\n",
      "  --bfile /Users/hn9/Documents/Analysis/FM-comparison/ukb_v3_downsampled10k/ukb_v3_chr19.downsampled10k\n",
      "  --chr 19\n",
      "  --from-bp 44408822\n",
      "  --maf 0.001\n",
      "  --out APOE_LDL_locus_UKBB.txt\n",
      "  --recode A\n",
      "  --to-bp 45408822\n",
      "\n",
      "16384 MB RAM detected; reserving 8192 MB for main workspace.\n",
      "6167 out of 364540 variants loaded from .bim file.\n",
      "10000 people (0 males, 0 females, 10000 ambiguous) loaded from .fam.\n",
      "Ambiguous sex IDs written to APOE_LDL_locus_UKBB.txt.nosex .\n",
      "Using 1 thread (no multithreaded calculations invoked).\n",
      "Before main variant filters, 10000 founders and 0 nonfounders present.\n",
      "Calculating allele frequencies... 10111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989 done.\n",
      "Total genotyping rate is 0.999988.\n",
      "0 variants removed due to minor allele threshold(s)\n",
      "(--maf/--max-maf/--mac/--max-mac).\n",
      "6167 variants and 10000 people pass filters and QC.\n",
      "Note: No phenotypes present.\n",
      "--recode A to APOE_LDL_locus_UKBB.txt.raw ... 101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='plink --bfile /Users/hn9/Documents/Analysis/FM-comparison/ukb_v3_downsampled10k/ukb_v3_chr19.downsampled10k --allow-extra-chr --recode A --chr 19 --from-bp 44408822 --to-bp 45408822 --maf 0.001 --out APOE_LDL_locus_UKBB.txt', returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PLINK command (placeholder using PLINK and UKBiobank LD reference panel)\n",
    "command = f\"plink --bfile /Users/hn9/Documents/Analysis/FM-comparison/ukb_v3_downsampled10k/ukb_v3_chr{target_chrom}.downsampled10k --allow-extra-chr --recode A --chr {target_chrom} --from-bp {start_pos} --to-bp {end_pos} --maf 0.001 --out {target}_locus_UKBB.txt\"\n",
    "subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o1112.legacyInferArrayTypeFromFirstElement. Trace:\npy4j.Py4JException: Method legacyInferArrayTypeFromFirstElement([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X32sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinished calculating LD correlation matrix\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X32sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ld_data, ld_matrix\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X32sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m ld_data, ld_matrix \u001b[39m=\u001b[39m get_ld_matrix_cor()\n",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X32sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calculates the LD matrix based on the LD data from PLINK using PySpark\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X32sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m ld_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m], size\u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m)), columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mE\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X32sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m ld_data \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(ld_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X32sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCalculating LD correlation matrix...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X32sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m vec_assembler \u001b[39m=\u001b[39m VectorAssembler(inputCols\u001b[39m=\u001b[39mld_data\u001b[39m.\u001b[39mcolumns, outputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data, columns\u001b[39m=\u001b[39mcolumn_names)\n\u001b[1;32m   1438\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m-> 1440\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(SparkSession, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[1;32m   1443\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:363\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    362\u001b[0m converted_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[1;32m   1486\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m-> 1093\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchemaFromList(data, names\u001b[39m=\u001b[39;49mschema)\n\u001b[1;32m   1094\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:953\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    949\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCANNOT_INFER_EMPTY_SCHEMA\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    950\u001b[0m         message_parameters\u001b[39m=\u001b[39m{},\n\u001b[1;32m    951\u001b[0m     )\n\u001b[1;32m    952\u001b[0m infer_dict_as_struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jconf\u001b[39m.\u001b[39minferDictAsStruct()\n\u001b[0;32m--> 953\u001b[0m infer_array_from_first_element \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jconf\u001b[39m.\u001b[39;49mlegacyInferArrayTypeFromFirstElement()\n\u001b[1;32m    954\u001b[0m prefer_timestamp_ntz \u001b[39m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m    955\u001b[0m schema \u001b[39m=\u001b[39m reduce(\n\u001b[1;32m    956\u001b[0m     _merge_type,\n\u001b[1;32m    957\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m     ),\n\u001b[1;32m    967\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o1112.legacyInferArrayTypeFromFirstElement. Trace:\npy4j.Py4JException: Method legacyInferArrayTypeFromFirstElement([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Finemapping Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def get_ld_matrix_cor():\n",
    "    \"\"\"Calculates the LD matrix based on the LD data from PLINK using PySpark\"\"\"\n",
    "    ld_data = pd.DataFrame(np.random.choice([0, 1], size=(5, 5)), columns=['A', 'B', 'C', 'D', 'E'])\n",
    "    ld_data = spark.createDataFrame(ld_data)\n",
    "\n",
    "    print('Calculating LD correlation matrix...')\n",
    "\n",
    "    vec_assembler = VectorAssembler(inputCols=ld_data.columns, outputCol=\"features\")\n",
    "    ld_data_vector = vec_assembler.transform(ld_data).select(\"features\")\n",
    "    ld_matrix = Correlation.corr(ld_data_vector, \"features\").collect()[0][0]\n",
    "\n",
    "    print('Finished calculating LD correlation matrix')\n",
    "    return ld_data, ld_matrix\n",
    "\n",
    "ld_data, ld_matrix = get_ld_matrix_cor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                             \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hn9/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/hn9/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/hn9/anaconda3/envs/finemapping_env/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinished calculating LD correlation matrix\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ld_data, ld_matrix\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m ld_data, ld_matrix \u001b[39m=\u001b[39m get_ld_matrix()\n",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 5\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m ld_data \u001b[39m=\u001b[39m ld_data\u001b[39m.\u001b[39mdrop(\u001b[39m*\u001b[39mdrop_list)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m ld_data\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     ld_data \u001b[39m=\u001b[39m ld_data\u001b[39m.\u001b[39;49mwithColumn(col, ld_data[col]\u001b[39m.\u001b[39;49mcast(\u001b[39m\"\u001b[39;49m\u001b[39mfloat\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCalculating LD correlation matrix...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m vec_assembler \u001b[39m=\u001b[39m VectorAssembler(inputCols\u001b[39m=\u001b[39mld_data\u001b[39m.\u001b[39mcolumns, outputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/dataframe.py:5170\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5166\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5167\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_COLUMN\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   5168\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcol\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(col)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m   5169\u001b[0m     )\n\u001b[0;32m-> 5170\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mwithColumn(colName, col\u001b[39m.\u001b[39;49m_jc), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_ld_matrix():\n",
    "    \"\"\"Calculates the LD matrix based on the LD data from PLINK using PySpark\"\"\"\n",
    "    # TODO debug matrix correlation in PySpark\n",
    "    # Doesn't finish running\n",
    "    ld_data = spark.read.csv(f\"{target}_locus_UKBB.txt.raw\", sep=\" \", header=True, inferSchema=True)\n",
    "    drop_list = [\"FID\", \"IID\", \"PAT\", \"MAT\", \"SEX\", \"PHENOTYPE\"]\n",
    "    ld_data = ld_data.drop(*drop_list)\n",
    "    for col in ld_data.columns:\n",
    "        ld_data = ld_data.withColumn(col, ld_data[col].cast(\"float\"))\n",
    "\n",
    "    print('Calculating LD correlation matrix...')\n",
    "\n",
    "    vec_assembler = VectorAssembler(inputCols=ld_data.columns, outputCol=\"features\")\n",
    "    ld_data_vector = vec_assembler.transform(ld_data).select(\"features\")\n",
    "    ld_matrix = Correlation.corr(ld_data_vector, \"features\").collect()[0][0]\n",
    "\n",
    "    print('Finished calculating LD correlation matrix')\n",
    "    return ld_data, ld_matrix\n",
    "\n",
    "ld_data, ld_matrix = get_ld_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/08 11:47:24 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 192.168.0.76:53025 in 10000 milliseconds\n",
      "23/10/08 11:47:37 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 13 more\n",
      "23/10/08 11:49:25 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 192.168.0.76:53025 in 10000 milliseconds\n",
      "23/10/08 11:49:50 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 192.168.0.76:53025 in 10000 milliseconds\n",
      "23/10/08 11:50:46 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 13 more\n",
      "23/10/08 11:51:24 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)\n",
      "23/10/08 11:51:45 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)\n",
      "23/10/08 11:51:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 192.168.0.76:53025 in 10000 milliseconds. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 192.168.0.76:53025 in 10000 milliseconds\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\n",
      "\t... 7 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hn9/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/hn9/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/hn9/anaconda3/envs/finemapping_env/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     ld_matrix_spark \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(ld_matrix_dict, schema\u001b[39m=\u001b[39mschema)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ld_data_spark, ld_matrix_spark\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m ld_data_spark, ld_matrix_spark \u001b[39m=\u001b[39m get_ld_matrix()\n",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ld_data_spark \u001b[39m=\u001b[39m ld_data_spark\u001b[39m.\u001b[39mdrop(\u001b[39m*\u001b[39mdrop_list)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m ld_data_spark\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     ld_data_spark \u001b[39m=\u001b[39m ld_data_spark\u001b[39m.\u001b[39;49mwithColumn(col, ld_data_spark[col]\u001b[39m.\u001b[39;49mcast(\u001b[39m\"\u001b[39;49m\u001b[39mfloat\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m column_names \u001b[39m=\u001b[39m ld_matrix\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m schema \u001b[39m=\u001b[39m StructType([StructField(name, FloatType(), \u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m column_names])\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/dataframe.py:5170\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5166\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5167\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_COLUMN\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   5168\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcol\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(col)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m   5169\u001b[0m     )\n\u001b[0;32m-> 5170\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mwithColumn(colName, col\u001b[39m.\u001b[39;49m_jc), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_ld_matrix():\n",
    "    # Calculate LD correlation\n",
    "    # TODO debug matrix correlation in PySpark\n",
    "    # Doesn't finish running\n",
    "    ld_data = pd.read_csv(f\"{target}_locus_UKBB.txt.raw\", delim_whitespace=True)\n",
    "    ld_data = ld_data.drop(columns=[\"FID\", \"IID\", \"PAT\", \"MAT\", \"SEX\", \"PHENOTYPE\"])\n",
    "    ld_matrix = ld_data.corr(method='pearson')\n",
    "    ld_data_spark = spark.read.csv(f\"{target}_locus_UKBB.txt.raw\", sep=\" \", header=True, inferSchema=True)\n",
    "    drop_list = [\"FID\", \"IID\", \"PAT\", \"MAT\", \"SEX\", \"PHENOTYPE\"]\n",
    "    ld_data_spark = ld_data_spark.drop(*drop_list)\n",
    "    for col in ld_data_spark.columns:\n",
    "        ld_data_spark = ld_data_spark.withColumn(col, ld_data_spark[col].cast(\"float\"))\n",
    "    column_names = ld_matrix.columns.tolist()\n",
    "    schema = StructType([StructField(name, FloatType(), True) for name in column_names])\n",
    "    ld_matrix_dict = ld_matrix.reset_index(drop=True).to_dict('records')\n",
    "    ld_matrix_spark = spark.createDataFrame(ld_matrix_dict, schema=schema)\n",
    "    return ld_data_spark, ld_matrix_spark\n",
    "ld_data_spark, ld_matrix_spark = get_ld_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ld_matrix():\n",
    "    # Calculate LD correlation\n",
    "    ld_data = pd.read_csv(f\"{target}_locus_UKBB.txt.raw\", delim_whitespace=True)\n",
    "    ld_data = ld_data.drop(columns=[\"FID\", \"IID\", \"PAT\", \"MAT\", \"SEX\", \"PHENOTYPE\"])\n",
    "    ld_matrix = ld_data.corr(method='pearson')\n",
    "    return ld_data, ld_matrix\n",
    "ld_data, ld_matrix = get_ld_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ld_data_spark = spark.read.csv(f\"{target}_locus_UKBB.txt.raw\", sep=\" \", header=True, inferSchema=True)\n",
    "drop_list = [\"FID\", \"IID\", \"PAT\", \"MAT\", \"SEX\", \"PHENOTYPE\"]\n",
    "ld_data_spark = ld_data_spark.drop(*drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hn9/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/08 13:31:32 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 165043 ms exceeds timeout 120000 ms\n",
      "23/10/08 13:31:32 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o101.legacyInferArrayTypeFromFirstElement. Trace:\npy4j.Py4JException: Method legacyInferArrayTypeFromFirstElement([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ld_matrix_spark \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(ld_matrix)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data, columns\u001b[39m=\u001b[39mcolumn_names)\n\u001b[1;32m   1438\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m-> 1440\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(SparkSession, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[1;32m   1443\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:363\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    362\u001b[0m converted_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[1;32m   1486\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m-> 1093\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchemaFromList(data, names\u001b[39m=\u001b[39;49mschema)\n\u001b[1;32m   1094\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:953\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    949\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCANNOT_INFER_EMPTY_SCHEMA\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    950\u001b[0m         message_parameters\u001b[39m=\u001b[39m{},\n\u001b[1;32m    951\u001b[0m     )\n\u001b[1;32m    952\u001b[0m infer_dict_as_struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jconf\u001b[39m.\u001b[39minferDictAsStruct()\n\u001b[0;32m--> 953\u001b[0m infer_array_from_first_element \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jconf\u001b[39m.\u001b[39;49mlegacyInferArrayTypeFromFirstElement()\n\u001b[1;32m    954\u001b[0m prefer_timestamp_ntz \u001b[39m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m    955\u001b[0m schema \u001b[39m=\u001b[39m reduce(\n\u001b[1;32m    956\u001b[0m     _merge_type,\n\u001b[1;32m    957\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m     ),\n\u001b[1;32m    967\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o101.legacyInferArrayTypeFromFirstElement. Trace:\npy4j.Py4JException: Method legacyInferArrayTypeFromFirstElement([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"
     ]
    }
   ],
   "source": [
    "ld_matrix_spark = spark.createDataFrame(ld_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m schema \u001b[39m=\u001b[39m StructType([StructField(name, FloatType(), \u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m column_names])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ld_matrix_dict \u001b[39m=\u001b[39m ld_matrix\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto_dict(\u001b[39m'\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hn9/Documents/GitHub/slalom-susie/python-pipeline-tests/pyspark_slalom_susie_pipeline.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ld_matrix_spark \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(ld_matrix_dict, schema\u001b[39m=\u001b[39;49mschema)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(SparkSession, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/sql/session.py:1487\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1485\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromLocal(\u001b[39mmap\u001b[39m(prepare, data), schema)\n\u001b[1;32m   1486\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1487\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39;49m_to_java_object_rdd())\n\u001b[1;32m   1488\u001b[0m jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsparkSession\u001b[39m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[39m.\u001b[39mrdd(), struct\u001b[39m.\u001b[39mjson())\n\u001b[1;32m   1489\u001b[0m df \u001b[39m=\u001b[39m DataFrame(jdf, \u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/rdd.py:4918\u001b[0m, in \u001b[0;36mRDD._to_java_object_rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4915\u001b[0m rdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pickled()\n\u001b[1;32m   4916\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 4918\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mpythonToJava(rdd\u001b[39m.\u001b[39;49m_jrdd, \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/rdd.py:5470\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5467\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5468\u001b[0m     profiler \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 5470\u001b[0m wrapped_func \u001b[39m=\u001b[39m _wrap_function(\n\u001b[1;32m   5471\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prev_jrdd_deserializer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd_deserializer, profiler\n\u001b[1;32m   5472\u001b[0m )\n\u001b[1;32m   5474\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   5475\u001b[0m python_rdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD(\n\u001b[1;32m   5476\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prev_jrdd\u001b[39m.\u001b[39mrdd(), wrapped_func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreservesPartitioning, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_barrier\n\u001b[1;32m   5477\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/finemapping_env/lib/python3.10/site-packages/pyspark/rdd.py:5270\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5268\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[39m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[1;32m   5269\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 5270\u001b[0m \u001b[39mreturn\u001b[39;00m sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mSimplePythonFunction(\n\u001b[1;32m   5271\u001b[0m     \u001b[39mbytearray\u001b[39;49m(pickled_command),\n\u001b[1;32m   5272\u001b[0m     env,\n\u001b[1;32m   5273\u001b[0m     includes,\n\u001b[1;32m   5274\u001b[0m     sc\u001b[39m.\u001b[39;49mpythonExec,\n\u001b[1;32m   5275\u001b[0m     sc\u001b[39m.\u001b[39;49mpythonVer,\n\u001b[1;32m   5276\u001b[0m     broadcast_vars,\n\u001b[1;32m   5277\u001b[0m     sc\u001b[39m.\u001b[39;49m_javaAccumulator,\n\u001b[1;32m   5278\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "column_names = ld_matrix.columns.tolist()\n",
    "schema = StructType([StructField(name, FloatType(), True) for name in column_names])\n",
    "ld_matrix_dict = ld_matrix.reset_index(drop=True).to_dict('records')\n",
    "ld_matrix_spark = spark.createDataFrame(ld_matrix_dict, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def get_sumstats(gwas_file_path, target_chrom, start_pos, end_pos):\n",
    "    sumstat = spark.read.csv(gwas_file_path, header=True, sep=\"\\t\")\n",
    "    sumstat = sumstat.filter(\n",
    "        (col('hm_chrom') == target_chrom) & \n",
    "        (col('hm_pos') >= start_pos) & \n",
    "        (col('hm_pos') <= end_pos)\n",
    "    )\n",
    "    sumstat = sumstat.dropna(subset=['hm_chrom'])\n",
    "\n",
    "    sumstat = sumstat.withColumn('z', col('beta') / col('standard_error'))\n",
    "\n",
    "    cols_to_rename = {\n",
    "        'hm_variant_id': 'ID',\n",
    "        'hm_rsid': 'rsid',\n",
    "        #'hm_chrom': 'chromosome', - chromosome col already exists\n",
    "        'hm_pos': 'position',\n",
    "        'hm_other_allele': 'allele1',\n",
    "        'hm_effect_allele': 'allele2',\n",
    "        'hm_effect_allele_frequency': 'maf',\n",
    "        'standard_error': 'se',\n",
    "        'p_value': 'p'\n",
    "    }\n",
    "    for old_name, new_name in cols_to_rename.items():\n",
    "        sumstat = sumstat.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    selected_cols = ['ID', 'rsid', 'chromosome', 'position', 'allele1', 'allele2', 'maf', 'p', 'beta', 'se', 'z']\n",
    "    sumstat = sumstat.select(selected_cols)\n",
    "    \n",
    "    return sumstat\n",
    "sumstat  = get_sumstats(gwas_file_path, target_chrom, start_pos, end_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_snps(sumstat, ld_matrix_spark, ld_data_spark):\n",
    "    \"\"\"Matches SNPs between summary statistics and LD matrix and filters them accordingly.\"\"\"\n",
    "    # Regex pattern for extracting position from SNP\n",
    "    pattern = re.compile(r\"(^\\d+)|(?<=:)\\d+(?=:|$)\")\n",
    "    ld_data_spark = ld_data_spark.withColumn(\"position\", F.regexp_extract(F.col(\"SNP\"), pattern, 0))\n",
    "    ld_data_spark = ld_data_spark.withColumn(\"ID\", F.regexp_replace(F.col(\"SNP\"), r'[:,_]', '_'))\n",
    "    \n",
    "    # Join sumstat with ld_data on 'ID'\n",
    "    concordance_test = sumstat.join(ld_data_spark, 'ID', 'inner')\n",
    "    \n",
    "    # Filter sumstat and ld_matrix for ID matches only\n",
    "    sumstat_filtered = sumstat.filter(F.col(\"ID\").isin(concordance_test.select(\"ID\").distinct().rdd.flatMap(lambda x: x).collect()))\n",
    "    ld_matrix_filtered = ld_matrix_spark.filter(F.col(\"ID\").isin(concordance_test.select(\"ID\").distinct().rdd.flatMap(lambda x: x).collect()))\n",
    "\n",
    "    return sumstat_filtered, ld_matrix_filtered, concordance_test\n",
    "\n",
    "sumstat_filtered, ld_matrix_filtered, concordance_test = match_snps(sumstat, ld_matrix, ld_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_snps(sumstat, ld_matrix, ld_data):\n",
    "    \"\"\"Matches SNPs between summary statistics and LD matrix and filters them accordingly.\"\"\"\n",
    "    # Getting only SNPs in sumstats that are also in the LD matrix\n",
    "    # Get SNP IDs from ld matrix to compare with sumstat SNP IDs\n",
    "    pattern = re.compile(r\"(^\\d+)|(?<=:)\\d+(?=:|$)\")\n",
    "    df1_transpose = ld_data.T.reset_index()\n",
    "    df1_transpose.columns = ['SNP'] + list(df1_transpose.columns[1:])\n",
    "    df1_transpose['position'] = df1_transpose['SNP'].apply(lambda x: re.search(pattern, x).group())\n",
    "    df1_transpose['ID'] = df1_transpose['SNP'].str.replace(r'[:,_]', '_').str.replace(r'_[^_]+$', '')\n",
    "    concordance_test = pd.merge(sumstat, df1_transpose, on='ID')\n",
    "\n",
    "    # Filter sumstat and LD matrix for ID matches only\n",
    "    sumstat_filtered = sumstat[sumstat['ID'].isin(concordance_test['ID'])]\n",
    "    sumstat_filtered.reset_index(drop=True, inplace=True)\n",
    "    ld_matrix_filtered = ld_matrix.loc[concordance_test['SNP'], concordance_test['SNP']]\n",
    "    return sumstat_filtered, ld_matrix_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allele_flip_check(snp_ids_38, sumstat_filtered):\n",
    "    df = spark.createDataFrame(snp_ids_38, StringType()).toDF(\"ID\")\n",
    "\n",
    "    # Split the 'ID' column to extract 'ref' and 'alt' columns\n",
    "    df = df.withColumn(\"ref_LD\", split(col(\"ID\"), \"_\")[2])\n",
    "    df = df.withColumn(\"alt_LD\", split(col(\"ID\"), \"_\")[3])\n",
    "\n",
    "    # Extract alleles using PySpark string functions\n",
    "    allele_df = concordance_test.withColumn('allele_parts', F.split('SNP', '[:,_]'))\n",
    "    concordance_test = allele_df.withColumn('allele1_LD', allele_df['allele_parts'].getItem(1)).\\\n",
    "                                 withColumn('allele2_LD', allele_df['allele_parts'].getItem(2))\n",
    "\n",
    "    # Join sumstat_filtered and concordance_test to align them\n",
    "    joint_df = sumstat_filtered.join(concordance_test, 'ID', 'inner')\n",
    "\n",
    "    # Flip z-scores if alleles are discordant\n",
    "    condition = (joint_df['ref'] != joint_df['ref_LD']) | (joint_df['alt'] != joint_df['alt_LD'])\n",
    "    sumstat_filtered = joint_df.withColumn('z', F.when(condition, -joint_df['z']).otherwise(joint_df['z']))\n",
    "    return sumstat_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dentist_calc(sumstat, target, lead_snp_ID, n_sample):\n",
    "    # 1. Getting R2 column for sumstats\n",
    "    ld = spark_session.read.csv(f'/Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/ld/{target}_subset_for_ld_calculation.ld', sep='\\\\s+', header=True)\n",
    "    lead_ld = ld.filter((ld['SNP_A'] == lead_snp_ID) | (ld['SNP_B'] == lead_snp_ID))\n",
    "    sumstat = spark_session.read.csv(f'/Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/{target}_locus_sumstat_flip_check.txt.gz', sep='\\t', header=True)\n",
    "    sumstat = sumstat.withColumn('ID', F.regexp_replace('ID', r\"_(\\d+)_([A-Z])_([A-Z])\", r\":\\1:\\2:\\3\"))\n",
    "    merged = lead_ld.join(sumstat.select('ID'), lead_ld['SNP_B'] == sumstat['ID'])\n",
    "    df = merged.select('ID', 'R2')\n",
    "    df = df.join(sumstat, 'ID', 'left_outer')\n",
    "    r_value = (n_sample * df.agg(F.sum('R2')).first()[0]) / (n_sample * df.agg(F.count('R2')).first()[0])\n",
    "    df = df.withColumn('r', F.lit(r_value))\n",
    "\n",
    "    lead_row = df.filter(df['ID'] == lead_snp_ID).first()\n",
    "    lead_z = lead_row['beta'] / lead_row['se']\n",
    "\n",
    "    # 2. Calculate 't_dentist_s' and 'dentist_outlier'\n",
    "    df = df.withColumn('t_dentist_s', (df['z'] - df['r'] * lead_z)**2 / (1 - df['r']**2))\n",
    "    df = df.withColumn('dentist_outlier', F.when((df['t_dentist_s'] < 1e-4) & (df['R2'] > 0.6), 1).otherwise(0))\n",
    "    df = df.drop('CHR_A', 'BP_A', 'SNP_A', 'CHR_B', 'BP_B', 'SNP_B')\n",
    "    df.write.csv(f'{target}_locus_sumstat_with_dentist.txt.gz', sep='\\t', header=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_data, ld_matrix = get_ld_matrix()\n",
    "sumstat = get_sumstats(gwas_file_path)\n",
    "sumstat_filtered, ld_matrix_filtered, concordance_test = match_snps(sumstat, ld_matrix, ld_data)\n",
    "sumstat_filtered = allele_flip_check(concordance_test, sumstat_filtered)\n",
    "\n",
    "lead_ld_command = f\"\"\"plink --bfile /Users/hn9/Documents/Analysis/FM-comparison/ukb_v3_downsampled10k/ukb_v3_chr{target_chrom}.downsampled10k \\\n",
    "        --allow-extra-chr \\\n",
    "        --r2 \\\n",
    "        --ld-snp {lead_snp_ID1} \\\n",
    "        --ld-window-kb 1000 \\\n",
    "        --ld-window 99999 \\\n",
    "        --ld-window-r2 0 \\\n",
    "        --out /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/ld/{target}_subset_for_ld_calculation\n",
    "\"\"\"\n",
    "subprocess.run(lead_ld_command, shell=True)\n",
    "\n",
    "df = dentist_calc(sumstat_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Reading summary statistics from file /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/APOE_LDL_locus_sumstat_with_dentist.txt.gz\n",
      "481 SNPs in summary statistics file\n",
      "Reading in LD matrix from file /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/APOE_LDL_locus_ukbb_ld.txt.gz\n",
      "Reading in LD matrix took 0.05 seconds\n",
      "Performing eigen decomposition\n",
      "Eigen decomposition took 0.06 seconds\n",
      "Running susieinf\n",
      "*********************************************************************\n",
      "* SuSiE-inf\n",
      "* Version 1.3\n",
      "* (C) Ran Cui, Zhou Fan\n",
      "*********************************************************************\n",
      "Iteration 0\n",
      "Update s^2 for effect 0 to 0.000006\n",
      "Update s^2 for effect 1 to 0.000006\n",
      "Update s^2 for effect 2 to 0.000006\n",
      "Update s^2 for effect 3 to 0.000006\n",
      "Update s^2 for effect 4 to 0.000006\n",
      "Update s^2 for effect 5 to 0.000006\n",
      "Update s^2 for effect 6 to 0.000006\n",
      "Update s^2 for effect 7 to 0.000006\n",
      "Update s^2 for effect 8 to 0.000006\n",
      "Update s^2 for effect 9 to 0.000006\n",
      "Update (sigma^2,tau^2) to (1.000038,0.000000e+00)\n",
      "Maximum change in PIP: 0.000209\n",
      "CONVERGED\n",
      "Running SuSiE-inf took 0.05 seconds\n",
      "Saving output to APOE_LDL_locus.susieinf.bgz\n",
      "Total time elapsed 0.30 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python /Users/hn9/Documents/GitHub/fine-mapping-inf/run_fine_mapping.py     --sumstats /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/APOE_LDL_locus_sumstat_with_dentist.txt.gz     --beta-col-name beta     --se-col-name se     --ld-file /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/APOE_LDL_locus_ukbb_ld.txt.gz     --n 94595     --method susieinf     --save-tsv     --eigen-decomp-prefix APOE_LDL_locus     --output-prefix  APOE_LDL_locus ', returncode=0)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "susieinf_command = f\"\"\"python /Users/hn9/Documents/GitHub/fine-mapping-inf/run_fine_mapping.py \\\n",
    "    --sumstats /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/{target}_locus_sumstat_with_dentist.txt.gz \\\n",
    "    --beta-col-name beta \\\n",
    "    --se-col-name se \\\n",
    "    --ld-file /Users/hn9/Documents/GitHub/fine-mapping-inf/susieinf/loci/{target}_locus_ukbb_ld.txt.gz \\\n",
    "    --n {n_sample} \\\n",
    "    --method susieinf \\\n",
    "    --save-tsv \\\n",
    "    --eigen-decomp-prefix {target}_locus \\\n",
    "    --output-prefix  {target}_locus \"\"\"\n",
    "\n",
    "subprocess.run(susieinf_command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='gunzip -c APOE_LDL_locus.susieinf.bgz > APOE_LDL_locus.txt', returncode=0)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_comamnd = f\"\"\"gunzip -c {target}_locus.susieinf.bgz > {target}_locus.txt\"\"\"\n",
    "subprocess.run(zip_comamnd, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
